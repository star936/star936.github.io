[{"title":"PySpark(二) 在IPython Notebook上使用Spark","url":"/2019/08/22/pyspark/pyspark_2/","content":"\n**上一篇博客: [PySpark(一): Hadoop SingleNode部署下Spark on yarn](https://star936.github.io/2019/08/21/pyspark/pyspark_1/)**\n\n## 1. 准备\n**1. 将Hadoop启动**\n**2. 安装:**\n* `Anaconda`\n* 创建虚拟环境\n```bash\nconda create -n venv python=2.7\n```\n* 安装Ipython Notebook\n```bash\nconda install ipython ipython-notebook\n```\n* 启用虚拟环境\n```bash\nsource active venv\n```\n## 2. 启动\n**使用如下命令:**\n```bash\nPYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" pyspark\n```\n\n## 3. 测试\n可以按照如图所示的命令测试是否成功:\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190822221953269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hhaXlhbmdnZW5n,size_16,color_FFFFFF,t_70)","tags":["PySpark"],"categories":["PySpark"]},{"title":"PySpark(一) Hadoop SingleNode部署下Spark on yarn","url":"/2019/08/21/pyspark/pyspark_1/","content":"\n> 环境: MacOX系统\n>  \t1. Java: 8\n>  \t2. Scala: 2.12.4\n>  \t3. Hadoop: 2.7.7\n>  \t4. Spark: 2.4.0\n\n\n## 1. 准备工作\n1. 安装Java, Scala, 并下载Spark及其相应版本的Hadoop;\n2. 编辑`~/.zshrc`\n```bash\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_151.jdk/Contents/Home\nexport HADOOP_HOME=/Users/dang/work/hadoop/hadoop-2.7.7\nexport SCALA_HOME=/Users/dang/work/hadoop/scala-2.12.4\nexport SPARK_HOME=/Users/dang/work/hadoop/spark-2.4.0\nexport PATH=$PATH:$SCALA_HOME/bin:$SPARK_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nexport JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH\n```\n\n## 2. 配置并启动Hadoop\n### 1. 配置\n\n1. 编辑`$HADOOP_HOME/etc/hadoop/core-site.xml`,并添加如下内容:\n```xml\n<configuration>\n<property>\n  <name>fs.defaultFS</name>\n  <value>hdfs://localhost:9000</value>\n</property>\n</configuration>\n```\n2. 编辑`$HADOOP_HOME/etc/hadoop/hdfs-site.xml`,并添加如下内容:\n```xml\n<configuration>\n<property>\n  <name>dfs.replication</name>\n  <value>1</value>\n</property>\n<property>\n  <name>dfs.namenode.name.dir</name>\n  <value>file:/Users/dang/work/hadoop/hadoop_data/hdfs/namenode</value>\n</property>\n<property>\n  <name>dfs.datanode.data.dir</name>\n  <value>file:/Users/dang/work/hadoop/hadoop_data/hdfs/datanode</value>\n</property>\n</configuration>\n```\n**注意: `dfs.namenode.name.dir`和`dfs.datanode.data.dir`目录在Hadoop启动前必须已存在**\n\n3. 编辑`$HADOOP_HOME/etc/hadoop/mapred-site.xml``,并添加如下内容:\n**首先执行`cp mapred-site.xml.template mapred-site.xml`**\n```xml\n<configuration>\n<property>\n  <name>mapreduce.framework.name</name>\n  <value>yarn</value>\n</property>\n</configuration>\n```\n4. 编辑`$HADOOP_HOME/etc/hadoop/yarn-site.xml`,并添加如下内容:\n```xml\n<configuration>\n<property>\n  <name>yarn.nodemanager.aux-services</name>\n  <value>mapreduce_shuffle</value>\n</property>\n<property>\n  <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n  <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n</property>\n<property>\n  <name>yarn.nodemanager.pmem-check-enabled</name>\n  <value>false</value>\n</property>\n\n<property>\n  <name>yarn.nodemanager.vmem-check-enabled</name>\n  <value>false</value>\n</property>\n<property>\n  <description>Whether to enable log aggregation</description>\n  <name>yarn.log-aggregation-enable</name>\n  <value>true</value>\n</property>\n</configuration>\n```\n5. 编辑`$HADOOP_HOME/etc/hadoop/hadoop-env.sh`,并添加如下内容:\n```bash\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_151.jdk/Contents/Home\n```\n\n### 3. 初始化HDFS\n执行如下命令:\n```bash\nhadoop namenode -format\n```\n### 3. 启动/停止\n执行如下命令:\n```bash\n# 启动\nstart-all.sh\n## 或者\nstart-dfs.sh # 先执行\nstart-yarn.sh # 后执行\n\n# 停止\nstop-all.sh\n## 或者\nstop-yarn.sh # 先执行\nstop-dfs.sh # 后执行\n```\n## 2. Spark配置并启动\n### 1. 配置\n1. 编辑`$SPARK_HOME/conf/spark-env.sh`,并添加如下内容:\n**首先执行 `cp spark-env.template spark-env.sh`**\n```bash\nexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_151.jdk/Contents/Home\nexport HADOOP_HOME=/Users/dang/work/hadoop/hadoop-2.7.7\nexport HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\nexport SPARK_HOME=/Users/dang/work/hadoop/spark-2.4.0\nexport SPARK_LOCAL_IP=\"127.0.0.1\"\nexport SPARK_LIBARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native\nexport LD_LIBRARY_PATH=$HADOOP_HOME/lib/native/:$LD_LIBRARY_PATH\n```\n2. 上传Spark Jar包到Hadoop HDFS\n```bash\nhadoop fs -put -p jars/ /hadoop/spark_jars\n```\n\n3. 编辑`$SPARK_HOME/conf/spark-defaults.conf`,并添加如下内容:\n**首先执行 `cp spark-defaults.conf.templatespark-defaults.conf`**\n```bash\nspark.yarn.jars hdfs://localhost:9000/hadoop/spark_jars/*\n```\n\n### 2.启动\n执行如下命令:\n```bash\npyspark --master yarn --deploy-mode client\n```\n\n## 4. 遇到的问题:\n```bash\nException from container-launch.\nContainer id: container_e64_1481762217559_27152_01_000002\nExit code: 127\n```\n**解决方法:**\n* 设置`hadoop-env.sh`中的`JAVA_HOME`\n* 设置`spark-env.sh`\n* Hadoop启动之后等几步分钟,让程序完全启动后,再去启动Spark","tags":["PySpark"],"categories":["PySpark"]},{"title":"Python之Pipenv使用","url":"/2019/06/24/python/pipenv_usage/","content":"\n\n> 工欲善其事,必先利其器.\n\n**Pipenv: Python Development Workflow for Humans**\n\n#### 1. 安装\n\n```bash\npip install pipenv\n```\n\n#### 2. 使用\n\n##### 2.1 创建虚拟环境\n\n**添加`--python`参数指定python版本号，前提条件是本地已经安装该版本的python.**\n\n```bash\npipenv --three/two\npipenv --python 2.7\npipenv --python 3.7\n```\n\n这会在项目目录中创建两个新文件：\n\n`Pipfile`:该文件是`TOML`格式，存放当前虚拟环境的配置信息，包括python版本，pypi源以及依赖包等，pipenv根据该文件寻找项目的根目录。\n\n`Pipfile.lock`:该文件是对Pipfile的锁定，支持锁定项目不同版本所依赖的环境。\n\n##### 2.2  activate与deactivate\n\n```bash\n# activate\npipenv shell\n# deactivate\nexit\n```\n\n##### 2.3 安装包\n\n**pipenv支持开发环境和生产环境依赖的分离。**\n\n`pipenv install`有多重作用：\n\n1. *如果虚拟环境已经存在，则安装Pipfile中的依赖包;*\n2. *如果虚拟环境不存在，但Pipfile存在，则根据Pipfile中python版本创建虚拟环境并安装依赖包;*\n3. *如果虚拟环境和Pipfile都不存在，则根据系统默认python版本创建虚拟环境.*\n\n```bash\npipenv install \n# 安装开发环境依赖(如py.tests,mock等)\npipenv install --dev\n# 指定包名\npipenv install [package_name]\n# 如果项目已经存在requirements.txt\npipenv install -r requirements.txt\n```\n\n**另外你也可以以下面格式的URL安装在git或其他版本控制系统中的包。**\n\n```bash\n<vcs_type>+<scheme>://<location>/<user_or_organization>/<repository>@<branch_or_tag>#<package_name>\n```\n\n**vcs_type有效值：**git，bzr，svn，hg\n\n**scheme有效值：**http，https，ssh，file\n\n**branch_or_tag：**可选参数\n\n**强烈建议以编辑模式安装任何版本控制依赖，如下示例：**\n\n```bash\n# 安装requests\npipenv install -e git+https://github.com/requests/requests.git@v2.19#egg=requests\n```\n\n\n\n#### 3. 常用命令\n\n**pipenv**\n\n```bash\npipenv [OPTIONS] COMMAND [ARGS]...\n```\n\n**Options:**\n\n - --where\n\n   输出项目根目录路径\n\n- --venv\n\n  输出虚拟环境信息\n\n- --envs\n\n  输出环境变量信息\n\n- --rm\n\n  删除当前虚拟环境\n\n- --pypi-mirror\n\n  指定PyPi的镜像\n\n- --site-packages\n\n  为虚拟环境启用site-packages\n\n**check**：检查包的安全性\n\n**clean**：卸载未在Pipfile.lock中指定的所有软件包\n\n**graph**：显示当前安装的依赖关系图信息\n\n**lock**：生成Pipfile.lock文件\n\n**run**：在未激活虚拟环境时可以直接使用虚拟环境的Python执行命令\n\n**sync**：安装所有在Pipfile.lock中指定的软件包\n\n**uninstall**：卸载指定的软件包并将其从`Pipfile`中删除\n\n**update**：更新指定包","tags":["Python","Pipenv"],"categories":["python"]},{"title":"使用Travis CI自动化发布Hexo博客","url":"/2019/06/18/hexo_travis/","content":"\n## 1. Travis登陆\n\n**打开[Travis Ci](https://www.travis-ci.org/),使用Github账号授权登录即可.**\n\n## 2. 准备\n\n**Hexo和Travis CI集成需要使用到GitHub的Token,处于安全考虑,我们需要对其进行加密.**\n\n*以下操作请在个人博客项目的根目录下进行*\n\n1. 安装Travis CI的命令行工具`travis`;\n2. 创建`.travis.yml`文件,\n   \n   ```bash\n   touch .travis.yml\n   ```\n\n3. 拷贝GitHub Token,并使用如下命令对其进行加密:\n   ```bash\n   travis encrypt 'GH_TOKEN=<GitHub Token>' --add\n   ```\n   此时在`.travis.yml`中存在如下内容:\n   \n   ```yaml\n    env:\n      global:\n        secure: <加密后的内容>\n   ```\n\n## 3. Hexo集成Travis CI\n\n**修改博客项目根目录下的`_config.yml`文件，修改内容如下：**\n\n```yaml\ndeploy:\n  type: git\n  repo: https://github.com/star936/star936.github.io.git  # 修改为自己的\n  branch: master\n```\n\n**向`.travis.yml`文件添加内容,完整内容示例如下:**\n\n```yaml\n# 指定语言环境\nlanguage: node_js\n# 指定需要sudo权限\nsudo: required\n# 指定node_js版本\nnode_js: stable\ngit:\n  submodules: false\nbefore_install:\n  - npm install -g hexo-cli\ninstall:\n  - npm install\nscript:\n  - git submodule init\n  - git submodule update\n  - hexo generate\nafter_success:\n  - git config --global user.name \"<GitHub用户名>\"\n  - git config --global user.email \"<GitHub邮箱>\"\n  - sed -i'' \"/^ *repo/s~github\\.com~${GH_TOKEN}@github.com~\" _config.yml\n  - hexo deploy\nbranches:\n  only:\n  - hexo\nenv:\n  global:\n    secure: <加密后的内容>\n```\n\n## 4. Build\n\n**将上述过程中产生的`.travis.yml`文件提交到代码库,即可触发Travis CI的build任务,并在成功build后将博客部署完成.**","tags":["Hexo","Travis"],"categories":["随笔"]},{"title":"使用Jenkins MultiBranch实现Docker build镜像并push到DockerHub","url":"/2018/12/30/spring/jenkins/","content":"\n\n> 使用Docker部署Jenkins，并通过MultiBranch功能自动对项目进行打包并docker build和push。\n\n\n\n#### 1. Docker部署Jenkins\n\n**拉取镜像**\n\n```bash\ndocker pull jenkinsci/jenkins:2.150.1\n```\n\n**启动**\n\n```bash\ndocker run --name jenkins -d -p 8080:8080 -p 50000:50000 -v ~/workspace/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v /usr/local/bin/docker:/bin/docker -t jenkinsci/jenkins:2.150.1\n```\n\n*其中：`-v /var/run/docker.sock:/var/run/docker.sock -v /usr/local/bin/docker:/bin/docker`作用是使用主机的docker命令。否则可能Jenkins创建docker镜像时会报`docker not found`错误。*\n\n**修改容器中docker权限**\n\n```bash\n# 进入Jenkins容器内\ndocker exec -it -u root 5c8 bash\n# 修改docker权限\nchmod 777 /var/run/docker.sock\n```\n\n*如果不修改容器中docker权限，则Jenkins进行到docker build时会报`dial unix /var/run/docker.sock: connect: permission denied`错误。*\n\n**Jenkins插件安装和创建multibranch job请自行完成。**\n\n#### 2. 编写Dockerfile\n\n```dockerfile\nFROM openjdk:8-jdk-alpine3.7\nVOLUME /tmp\n\nARG JAR_FILE\nADD target/${JAR_FILE} target/app.jar\n\nEXPOSE 8080\n\nRUN touch target/app.jar\nENTRYPOINT [\"java\",\"-jar\",\"target/app.jar\"]\n```\n\n#### 3. 编写Jenkinsfile\n\n```groovy\nnode {\n    def app\n    def mvnHome = tool 'maven'\n    env.PATH = \"${mvnHome}/bin:${env.PATH}\"\n    def IMAGE_NAME = \"star936/rest-api-mybatis\"\n\n    stage('Package') {\n        /* Let's make sure we have the repository cloned to our workspace */\n\n        checkout scm\n        sh 'mvn -v'\n        sh 'mvn clean package -Dmaven.test.skip=true -Ddockerfile.skip=true -P dev'\n    }\n\n    stage('Build and Push image') {\n        /* This builds the actual image; synonymous to\n         * docker build on the command line */\n        /* Finally, we'll push the image with two tags:\n         * First, the incremental build number from Jenkins\n         * Second, the 'latest' tag.\n         * Pushing multiple tags is cheap, as all the layers are reused. */\n        echo env.BRANCH_NAME\n        echo env.WORKSPACE\n        sh 'docker version'\n        def JAR_FILE = IMAGE_NAME + '.jar'\n        docker.withRegistry('https://registry.hub.docker.com', 'dockerhub') {\n            app = docker.build(\"${env.IMAGE_NAME}:${BRANCH_NAME}\", \"--build-arg JAR_FILE=${JAR_FILE} .\")\n            app.push()\n        }\n    }\n}\n```\n\n**`tool 'maven'`中的`maven`是在`Jenkins->系统管理->全局工具配置->Maven->Add Maven`处填写的名字.**\n\n**说明：`Dockerfile`和`Jenkinsfile`中出现的`JAR_FILE`参数，与`pom.xml`的`JAR_FILE`相互应。**\n\n```xml\n<plugin>\n    <groupId>com.spotify</groupId>\n    <artifactId>dockerfile-maven-plugin</artifactId>\n    <version>${dockerfile-maven-version}</version>\n    <configuration>\n        <useMavenSettingsForAuth>true</useMavenSettingsForAuth>\n        <repository>${docker.image.prefix}/${docker.repository.name}</repository>\n        <tag>${project.version}</tag>\n        <buildArgs>\n            <JAR_FILE>${project.build.finalName}.jar</JAR_FILE>\n        </buildArgs>\n    </configuration>\n</plugin>\n```\n\n#### 4. 配置GitHub Webhooks\n\n**想要实现git push后Jenkins自动构建变化的分支,则还需要配置`GitHub Webhooks`: 在项目的`settings->Webhooks->Add webhook->Payload URL`处填写`http://{jenkins服务器IP}/git/notifyCommit?url={MultiBranch添加repository源时填写的URL}&delay=0sec`**\n\n*对于在本地搭建Jenkins的,推荐使用[ngrok](https://ngrok.com/).*\n\n","tags":["Jenkins"],"categories":["Spring"]},{"title":"使用Dokcer+SpringCloud+Consul+Fabio搭建微服务","url":"/2018/12/21/spring/consul_fabio/","content":"\n> SpringBoot: 2.1.1\n> SpringCloud: Greenwich.M3\n> Consul: consul:1.3.0\n> Fabio: fabiolb/fabio:1.5.5-go1.9.2\n\n**具体的pom.xml和application-dev.yml文件内容可以参考[我的GitHub项目](https://github.com/star936/rest-api-mybatis)**\n\n### 1. Consul\n#### 1.1 安装\n```bash\ndocker pull consul:1.3.0\n```\n#### 1.2 启动\n```bash\ndocker run -d -t -p 8300:8300 -p 8301:8301 -p 8302:8302 -p 8500:8500 -p 8600:8600 --name test-consul consul:1.3.0\n```\n\n### 2. Fabio\n#### 2.1 安装\n```bash\ndocker pull fabiolb/fabio:1.5.5-go1.9.2\n```\n#### 2.2 启动\n```bash\ndocker run -d -t -p 9998:9998 -p 9999:9999 --link test-consul:consul -e 'registry_consul_addr=consul:8500' fabiolb/fabio:1.5.5-go1.9.2\n```\n\n### 3. Docker部署SpringBoot应用\n#### 3.1 打包\n**请参考[使用Docker打包SpringBoot并Push到DockerHub](/2018/12/19/spring/docker)文章中的打包.并对pom.xml和application-dev.yml、application-test.yml进行如下修改:**\n\n*在pom.xml中添加如下内容:*\n```xml\n<!--<properties>标签中-->\n<spring-cloud.version>Greenwich.M3</spring-cloud.version>\n\n<!--<dependencies>标签中-->\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-actuator</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-consul-discovery</artifactId>\n</dependency>\n<dependency>\n    <groupId>org.springframework.cloud</groupId>\n    <artifactId>spring-cloud-starter-openfeign</artifactId>\n</dependency>\n<dependency>\n    <groupId>com.netflix.feign</groupId>\n    <artifactId>feign-httpclient</artifactId>\n    <version>8.18.0</version>\n</dependency>\n\n<!--与<dependencies>标签同级-->\n<dependencyManagement>\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-dependencies</artifactId>\n            <version>${spring-cloud.version}</version>\n            <type>pom</type>\n            <scope>import</scope>\n        </dependency>\n    </dependencies>\n</dependencyManagement>\n<repositories>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>https://repo.spring.io/milestone</url>\n        <snapshots>\n            <enabled>false</enabled>\n        </snapshots>\n    </repository>\n</repositories>\n```\n*在application-dev.yml中添加如下内容:*\n```yaml\nspring:\n  application:\n    name: rest-api-mybatis\n  cloud:\n    consul:\n      host: 127.0.0.1\n      port: 8500\n      discovery:\n        register: true\n        tags: urlprefix-/rest strip=/rest\n```\n#### 3.2 启动\n```bash\ndocker run --name rest_api --net=host -t star936/rest-api-mybatis:0.0.1\n```\n* -t参数: Docker打包应用的`镜像名:tag`\n\n### 4. 验证\n#### 4.1 查看consul ui\n**访问http://localhost:8500,查看是否注册上服务,如下图所示:**\n![图1](consul_fabio/1.png)\n\n#### 4.2 请求\n**向 http://localhost:9999/rest/{controller的URL} 发送请求,如果返回数据,则表示成功.**\n\n\n\n\n","tags":["SpringBoot","Docker"],"categories":["Spring"]},{"title":"使用Docker打包SpringBoot并Push到DockerHub","url":"/2018/12/20/spring/docker/","content":"\n> SpringBoot: 2.1.1\n> com.spotify.dockerfile-maven-plugin: 1.4.8\n\n\n#### 1. Docker打包\n##### 1.1 \b修改application.yml\n```yaml\nspring:\n  profiles:\n    active: @spring.profiles.active@\n```\n** ` @spring.profiles.active@`中的`pring.profiles.active`与`pom.xml->profiles->profile->properties->spring.profiles.active`标签名称一致.\b**\n**对于多环境的配置文件,SpringBoot提供了Profile功能来,因此在项目的`resources`目录下创建`application-{profile的名称(与pom.xml->profile->id属性值保持一致)}.yml(或.properties)`格式文件,并在打包或者运行时指定Profile即可.配置文件的名称,参考如下图所示: **\n![图1](docker/1.png)\n*\b表明只有两个profile:dev和test*\n\n##### 1.2 Dockerfile\n```Dockerfile\nFROM openjdk:8-jdk-alpine3.7\nVOLUME /tmp\n\nARG JAR_FILE\nADD target/${JAR_FILE} target/app.jar\n\nEXPOSE 8080\n\nRUN touch target/app.jar\nENTRYPOINT [\"java\",\"-jar\",\"target/app.jar\"]\n```\n\n##### 1.3 配置pom.xml\n**添加如下内容:**\n```xml\n\t<properties>\n\t\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n\t\t<project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n\t\t<java.version>1.8</java.version>\n        <dockerfile-maven-version>1.4.8</dockerfile-maven-version>\n        <docker.image.prefix>star936</docker.image.prefix>\n        <docker.repository.name>rest-api-mybatis</docker.repository.name>\n\t</properties>\n\n    <build>\n        <finalName>${docker.image.prefix}/${docker.repository.name}</finalName>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n                <configuration>\n                    <!--fork :  如果没有该项配置，肯定devtools不会起作用，即应用不会restart -->\n                    <fork>true</fork>\n                </configuration>\n            </plugin>\n            <plugin>\n                <groupId>com.spotify</groupId>\n                <artifactId>dockerfile-maven-plugin</artifactId>\n                <version>${dockerfile-maven-version}</version>\n                <executions>\n                    <execution>\n                        <id>build-image</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>build</goal>\n                            <goal>tag</goal>\n                        </goals>\n                        <configuration>\n                        </configuration>\n                    </execution>\n                    <execution>\n                        <id>push-image</id>\n                        <phase>deploy</phase>\n                        <goals>\n                            <goal>push</goal>\n                        </goals>\n                    </execution>\n                </executions>\n                <configuration>\n                    <useMavenSettingsForAuth>true</useMavenSettingsForAuth>\n                    <repository>${docker.image.prefix}/${docker.repository.name}</repository>\n                    <tag>${project.version}</tag>\n                    <buildArgs>\n                        <JAR_FILE>${project.build.finalName}.jar</JAR_FILE>\n                    </buildArgs>\n                </configuration>\n            </plugin>\n        </plugins>\n        <resources>\n            <resource><!-- 扫描替换 -->\n                <directory>src/main/resources</directory>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n    </build>\n    <profiles>\n        <!-- 测试环境 -->\n        <profile>\n            <id>test</id>\n            <activation>\n                <activeByDefault>true</activeByDefault>\n            </activation>\n            <properties>\n                <spring.profiles.active>test</spring.profiles.active>\n            </properties>\n        </profile>\n        <!-- 开发环境 -->\n        <profile>\n            <id>dev</id>\n            <activation>\n                <activeByDefault>false</activeByDefault>\n            </activation>\n            <properties>\n                <spring.profiles.active>dev</spring.profiles.active>\n            </properties>\n        </profile>\n    </profiles>\n```\n\n##### 1.2 打包\n**参照下图:**\n![图2](docker/2.png)\n* 第一步: 勾选`Profile`;\n* 第二步: 选中`Lifecycle->clean`并运行;\n* 第三步: 选中`Lifecycle->package`并运行.\n\n#### 2. Push镜像到DockerHub私有仓库\n##### 2.1 配置Maven的setting.xml\n**\b添加如下内容:**\n```xml\n<servers>\n    <server>\n        <id>docker.io</id>\n        <username>DockerHub用户名</username>\n        <password>DockerHub密码</password>\n        <configuration>\n        <email>DockerHub注册邮箱</email>\n        </configuration>\n    </server>\n</servers>\n```\n##### 2.2 Push\n**参照上图选中`Plugins->dockerfile->dockerfile:push`并运行.**\n","tags":["SpringBoot","Docker"],"categories":["Spring"]},{"title":"Spring(一):IoC","url":"/2018/12/17/spring/ioc/","content":"\n> SpringBoot: 2.1.1\n\n### 1. IoC简介\n\n**IoC容器是Spring的核心,可以说Spring是一种基于IoC容器编程的框架,Spring Boot是基于注解的开发Spring IoC.**\n\n**IoC是一种通过描述来生成或者获取对象的技术,该技术不是Spring设置也不是Java所独有的.**\n\n**在Spring中把每一个需要管理的对象称为Spring Bean,而Spring管理这些Bean的容器称之为Spring IoC容器,所有的IoC容器都需要实现接口`BeanFactory`.它具有两个基本的功能:**\n\n* 通过描述管理Bean,包括发布和获取Bean\n* 通过描述完成Bean之间的依赖关系\n\n### 2. 装配Bean\n\n#### 2.1 通过扫描装配Bean\n\n使用注解`@Component`和`@ComponentScan`进行扫描装配,其中`@Component`表明标明哪个类被扫描进入Spring IoC容器,`@ComponentScan`标明采用何种策略去扫描装配Bean. \n\n**注解@SpringBootApplication注入了@ComponentScan**\n\n```java\n# 注解@Component表明User类将被IoC容器扫描装配,\"user\"作为Bean的名称.\n@Component(\"user\")  \npublic class User {\n    private Long id;\n    private String userName;\n    ...\n}\n\n# @Cofiguration代表这是一个Java配置文件,Spring的容器会根据它来生成IoC容器去装配Bean\n# @ComponentScan默认对AppConfig的当前包及其子包进行扫描去装配Bean \n@Configuration\n@ComponentScan\npublic class AppConfig {\n}\n```\n\n**@ComponentScan的常用属性:**\n\n* basePackages: 定义扫描的包名\n* basePackageClasses: 定义扫描的类\n* includeFilters: 定义满足过滤器条件的Bean才去扫描\n* excludeFilters: 排除满足过滤器条件的Bean\n\n**`includeFilters`和`excludeFilters`都需要通过一个注解`@Filter`去定义,`@Filter`有一个`type`类型,可以定义为注解或者正则式等类型,classes定义注解类,pattern定义正则式类.**\n\n*例如: 将标注了`@Service`的类不被IoC容器扫描注入*\n\n```java\n@ComponentScan(basePackages=\"com.morven.rest.*\", excludeFilters = (@Filter(classes = {Service.class})))\n```\n\n#### 2.2 自定义第三方Bean\n\n**如果希望将第三方包的类对象也注入到Spring IoC容器中,使用@Bean注解.**\n\n```java\n@Configuration\npublic class AppConfig {\n    # 通过@Bean会将该函数返回的对象用名称\"dataSource\"保存在IoC容器中\n    @Bean(name=\"dataSource\")\n    public DataSource getDataSource() {\n        ...\n    }\n}\n```\n\n### 3. 依赖注入(Dependency Injection, DI)\n\n**描述的是Bean之间的依赖关系**\n\n*例如:*\n\n```java\npublic interface Person {\n    public void service();\n}\npublic interface Animal {\n    public void use();\n}\n@Component\npublic class BussinessPerson implements Person {\n    @Autowired\n    private Anminal animal = null;\n    @Override\n    public void service() {\n        this.animal.use();\n    }\n}\n\n@Component\npublic class Dog implements Animal {\n    @Override\n    public void use() {\n        System.out.println(\"Dog...\");\n    }\n}\n```\n\n#### 3.1 注解@Autowired\n\n**使用的最多的注解之一,它注入的机制最基本的一条是根据类型.它既可以标注属性,也可以标注方法.**\n\n*如果我们再添加一个Animal的实现类Cat*\n\n```java\n@Component\npublic class Cat implements Animal {\n    @Override\n    public void use() {\n        System.out.println(\"Cat...\")\n    }\n}\n```\n\n*抛出异常:*\n\n```bash\nexpected single matching bean but found 2: cat, dog\n```\n\n*修改:*\n\n```java \n@Autowired\nprivate Animal dog = null;\n```\n\n**原因: @Autowired首先根据类型找到对应的Bean,如果对应类型的Bean不是唯一的,那么它会根据其属性名称和Bean的名称进行匹配;如果匹配上就使用该Bean,否则抛出异常.**\n\n**另外: @Autowired是一个默认必须找到对应Bean的注解,如果不确定其标注属性一定存在并且允许被标注的属性为null,那么可以如下配置**:\n\n```java\n@Autowired(required = false)\n```\n\n#### 3.2 使用@Primary和@Qualifier消除歧义性\n\n*由于接口Animal有两个实现类,因此造成@Autowired装配的歧义性,`3.1`中给出了一种解决方法,但不是最好的方案;而@Primary和@Qualifier给出了更好的解决方案.*\n\n* @Primary: 是一个修改优先权的注解,通过优先级的变换使得IoC容器知道注入哪个具体的实例来满足依赖注入.如下:\n\n  ```java\n  @Component\n  # 优先使用该类进行注入\n  @Primary\n  public class Cat implements Animal {\n      ...\n  }\n  ```\n\n* @Qualifier: 有时候Dog和Cat class都带有@Primary注解,那么@Autowired和@Qualifier组合在一起,通过类型和名称一起找到Bean,以此来消除歧义性.如下:\n\n  ```java\n  @Autowired\n  @Qualifier(\"dog\")\n  private Anminal animal = null;\n  ```\n\n#### 3.3 带有参数的构造方法类的装配\n\n*如果有些类只有带有参数的构造方法,则可以使用如下方法::*\n\n```java\n@Component\npublic class BussinessPerson implements Person {\n    private Anminal animal = null;\n    \n    public BussinessPerson(@Autowired @Qualifier(\"dog\") Animal animal) {\n        this.animal = animal;\n    }\n    \n    @Override\n    public void service() {\n        this.animal.use();\n    }\n}\n```\n\n","tags":["Spring"],"categories":["Spring"]},{"title":"FastJson的使用","url":"/2018/12/15/spring/fastjson/","content":"\n> Springboot: 2.1.1\n>\n> FastJson: 1.2.47\n\n> 最近项目中需要使用到Springboot,因此需要边学边写边记载下.\n\n\n\n### FastJson的使用\n\n#### 1. 替换Jackson\n\n**在Springboot2中数据的序列化和反序列化默认使用的是Jackson,因此需要替换该用FastJson.**\n\n```java\npackage com.example.morven.config;\n\nimport com.alibaba.fastjson.serializer.SerializerFeature;\nimport com.alibaba.fastjson.support.config.FastJsonConfig;\nimport com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.http.converter.HttpMessageConverter;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurationSupport;\n\nimport java.util.List;\n\n@Configuration\npublic class WebAppConfig extends WebMvcConfigurationSupport {\n    /**\n     * Convert message using FastJson.\n     * @param converters List<HttpMessageConverter<?>\n     */\n    @Override\n    protected void configureMessageConverters(List<HttpMessageConverter<?>> converters) {\n        super.configureMessageConverters(converters);\n        FastJsonHttpMessageConverter fastConverter = new FastJsonHttpMessageConverter();\n        FastJsonConfig fastJsonConfig = new FastJsonConfig();\n        fastJsonConfig.setSerializerFeatures(\n                SerializerFeature.PrettyFormat\n        );\n        fastConverter.setFastJsonConfig(fastJsonConfig);\n    }\n}\n```\n\n#### 2. 注解的使用\n\n##### 2.1 @JSONField\n\n*既可以在属性上使用也可以在getter/getter方法上使用.*\n\n常用属性:\n\n* name: 指定序列化和反序列化时的字段名称\n* format: 指定时间的格式\n* serialize: Boolean类型,是否序列化该字段\n* deserialize: Boolean类型,是否反序列化该字段\n* serializeUsing: 指定属性的序列化类\n* deserializeUsing: 指定属性的反序列化类\n\n##### 2.2 @JSONType\n\n*在类上使用.*\n\n常用属性:\n\n* includes: 指定序列化和反序列化的字段\n* ignores: 指定序列化和反序列化忽略的字段\n\n#### 3. 思考\n\n* **Java Bean和数据库表映射,如果我们想要在序列化或反序列化时包含一些别的属性值,那么该怎么办?**\n\n**使用JSONField注解的serialize/deserialize属性.**\n\n* **对于字段`name`属性,反序列化时使用名称`user_name`,序列化时使用名称`contact_name`,该怎么办?**\n\n**在`name`的setter方法上使用@JSONField(name=\"user_name\"),在getter方法上使用@JSONField(name=\"contract_name\").**\n\n#### 4. 循环引用\n**情景:**\n*例如: 存在用户-订单的一对多关系,如果查询每个订单的时候都要返回用户信息.*\n*查询所用订单信息时,使用Mybatis的@One注解,此时查询出来的结果使用fastjson序列化后结果中存在大量的类似`\"$ref\": \"$[0]\"`的信息.*\n**问题分析:**\n*在查询时,当多个订单的用户是同一个人时,都指向的是内存中的同一个用户对象,而fastjson默认开启循环引用检测,因此结果中会出现上述描述的情形.*\n**解决方法:**\n```java\nJSON.toJSONString(object, SerializerFeature.DisableCircularReferenceDetect);\n```","tags":["fastjson"],"categories":["Spring"]},{"title":"Blaze(七):URI strings","url":"/2018/12/05/blaze/uri_strings/","content":"\n\nBlaze使用strings指定数据源，使用时非常简单。\n\n#### 1. 例子\n\n*与一组CSV文件或一个SQL数据库交互*\n\n```python\n# coding: utf-8\n\nfrom blaze import *\nfrom blaze.utils import example\n\nt = data(example('accounts_*.csv'))\nprint(t.peek())\n\nt1 = data('sqlite:///%s::iris' % example('iris.db'))\nprint(t1.peek())\n```\n\n*迁移CSV文件数据到SQL数据库*\n\n```python\nfrom blaze.utils import example\nfrom odo import odo\n\nprint(odo(example('iris.csv'), 'sqlite:///myfile.db::iris'))\n```\n\n**Blaze支持的URIs的种类：**\n\n* 磁盘上文件的路径，包含如下扩展：\n  * `.csv`\n  * `.json`\n  * `.csv.gz/json.gz`\n  * `.hdf5`(使用h5py)\n  * `.hdf5::/datapath`\n  * `hdfstore://filename.hdf5`(使用pandas.HDFStore格式)\n  * `.bcolz`\n  * `.xls(x)`\n\n* 如下的SQLAlchemy strings\n\n  * `sqlite:////absolute/path/to/myfile.db::tablename`\n  * `sqlite:////absolute/path/to/myfile.db`\n  * `postgresql://username:password@hostname:port`\n  * `impala://hostname`(使用impala)\n  * 其余被SQLAlchemy支持的\n\n* 如下格式的MongoDB连接strings\n\n  `mongodb://username:password@hostname:port/database_name::collection_name`\n\n* 如下格式的Blaze server strings\n\n  `blaze://hostname:port`(默认端口号6363)\n\n*如果对于传统的URI需要额外的一个位置或者一个表名，可以在URI末尾使用`::`分割并跟在其后面。*\n\n#### 2. 如何工作的\n\n**Blaze依赖于`odo`库来处理URI。URI是由`resource`函数管理的，而`resource`函数是基于正则表达式分发的。例如下面处理.json文件的一个简单的resource函数（尽管Blaze实际的解决方法比它更全面）：** \n\n```python\nfrom blaze import resource\nimport json\n\n@resource.register('.+\\.json')\ndef resource_json(uri):\n    with open(uri):\n        data = json.load(uri)\n    return data\n```\n\n#### 3. 可以扩展到自己的类型吗？\n\n**当然可以。正如`2. 如何工作的`部分所展示的那样导入和扩展resource，剩下的blaze将自动接受你的更改。**","tags":["Python","Blaze"],"categories":["Blaze"]},{"title":"Blaze(六):Pandas与Blaze比较","url":"/2018/12/05/blaze/compare_to_pandas/","content":"\n**导入和构造**\n\n```python\n# coding: utf-8\n\nimport numpy as np\nimport pandas as pd\nfrom blaze import data, by\n\ndf = pd.DataFrame({'name': ['Alice', 'Bob', 'Joe', 'Bob'],\n                   'amount': [100, 200, 300, 400],\n                   'id': [1, 2, 3, 4],\n                   })\n\ndf = data(df)\nprint(df.peek())\n```\n\n*输出：*\n\n```bash\n    name  amount  id\n0  Alice     100   1\n1    Bob     200   2\n2    Joe     300   3\n3    Bob     400   4\n```\n| Computaion              | Pandas                                                       | Blaze                                                        |\n| ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Column Arithmetic       | df.amount * 2                                                | df.amount * 2                                                |\n| Multiple Columns        | df[['id', 'amount']]                                         | df[['id', 'amount']]                                         |\n| Selection               | df[df.amount > 300]                                          | df[df.amount > 300]                                          |\n| Group By                | df.groupby('name').amount.mean() df.groupby(['name','id']).amount.mean() | by(df.name, amount=df.amount.mean()) by(merge(df.name, df.id), amount=df.amount.mean()) |\n| Join                    | pd.merge(df, df2, on='name')                                 | join(df, df2, 'name')                                        |\n| Map                     | df.amount.map(lambda x: x + 1)                               | df.amount.map(lambda x: x + 1, 'int64')                      |\n| Relabel Columns         | df.rename(columns={'name': 'alias', 'amount': 'dollars'})    | df.relabel(name='alias', amount='dollars')                   |\n| Drop duplicates         | df.drop_duplicates() df.name.drop_duplicates()               | df.distinct() df.name.distinct()                             |\n| Reductions              | df.amount.mean() df.amount.value_counts()                    | df.amount.mean() df.amount.count_values()                    |\n| Concatenate             | pd.concat((df, df))                                          | concat(df, df)                                               |\n| Column Type Information | df.dtypes df.amount.dtype                                    | df.dshape df.amount.dshape                                   |\n\n**Blaze可以简化一些常见的IO任务，并使其更具可读性，这些任务是希望使用pandas处理的。这些例子使用的是`odo`库。许多情况下，blaze可以处理超出内存大小的数据集，而这是pandas不能够容易处理的事情。**\n\n```python\nfrom odo import odo\n```\n\n| Operations                  | Pandas                                                       | Blaze                                    |\n| --------------------------- | ------------------------------------------------------------ | ---------------------------------------- |\n| Load directory of CSV files | df = pd.concat([pd.read_csv(filename)                 for filename in                 glob.glob('path/to/*.csv')]) | df = data('path/to/*.csv')               |\n| Save result to CSV file     | df[df.amount < 0].to_csv('output.csv')                       | odo(df[df.amount < 0],     'output.csv') |\n| Read from SQL database      | df = pd.read_sql('select * from t', con='sqlite:///db.db')  df = pd.read_sql('select * from t',      con=sa.create_engine('sqlite:///db.db')) | df = data('sqlite://db.db::t')           |\n\n","tags":["Python","Blaze"],"categories":["Blaze"]},{"title":"Blaze(五):数据的分割-应用-组合-分组","url":"/2018/12/05/blaze/group/","content":"\n*分组操作将一张表切分为多个块，并对每个块进行操作。*\n\n*以species分组，并对petal求平均值*\n\n```python\n# coding: utf-8\n\nfrom blaze import data, by\nfrom blaze.utils import example\n\nd = data('sqlite:///{}::iris'.format(example('iris.db')))\nprint(by(d.species, avg=d.petal_length.mean()).peek())\n```\n\n*对于描述许多有用的变换，split-apply-combine操作是一种简单而强大的方式。它被所有后端高效支持。*\n\n#### 1. 参数\n\n`by`方法有一个位置参数(决定如何对表进行分组)和多个关键字参数(对每一个份组执行的操作)。形式如下：\n\n```python\nby(grouper, name=reduction, name=reduction, ...)\n```\n\n*例子：以species对iris数据集分组，并对每个分组计算petal_length的最小值、最大值和最大值与最小值间的差。*\n\n```python\nprint(by(d.species, min=d.petal_length.min(), max=d.petal_length.max(),\n         ratio=d.petal_length.max()-d.petal_length.min()).peek())\n```\n\n*输出：*\n\n```bash\n           species  max  min  ratio\n0      Iris-setosa  1.9  1.0    0.9\n1  Iris-versicolor  5.1  3.0    2.1\n2   Iris-virginica  6.9  4.5    2.4\n```\n\n#### 2. 限制\n\n与内存型的dataframes例如`pandas`或`dplyr`相比，这个接口有两个方面的限制：\n\n1. 必须同时指定分组方式和相应操作\n2. “apply”阶段必须是一个reduction\n\n这些限制让它更方便向数据集传达你的意图，并有效分发和并行计算。\n\n#### 3. 不能做的事\n\n**不能只指定如何分组而没有指定对每个分组的操作，例如：**\n\n```python\ngroups = by(mytable.mycolumn)  # Can't do this  \n```\n\n**不能指定non-reducing的apply操作(尽管有些后端通过改变而正常工作),例如：**\n\n```python\ngroups = by(d.A, result=d.B / d.B.max())  # Can't do this\n```\n\n","tags":["Python","Blaze"],"categories":["Blaze"]},{"title":"Blaze(四):基本查询","url":"/2018/12/04/blaze/basic_query/","content":"\n> 使用之前x下载的iris数据集CSV文件.\n\n*该段代码以下所有示例都会使用到*\n```python\n# coding: utf-8\n\nfrom blaze import data\nfrom blaze.utils import example\n\niris = data(example('iris.csv'))\nprint(iris.peek())\n```\n*输出:*\n```bash\n    sepal_length  sepal_width  petal_length  petal_width      species\n0            5.1          3.5           1.4          0.2  Iris-setosa\n1            4.9          3.0           1.4          0.2  Iris-setosa\n2            4.7          3.2           1.3          0.2  Iris-setosa\n3            4.6          3.1           1.5          0.2  Iris-setosa\n...\n```\n\n#### 1. 获取列数据\n**两种方式获取单独列的数据:**\n```python\n# 第一种\nprint(iris.species.peek())\n# 第二种\nprint(iris['species'].peek())\n```\n*输出:*\n```bash\n        species\n0   Iris-setosa\n1   Iris-setosa\n2   Iris-setosa\n3   Iris-setosa\n4   Iris-setosa\n5   Iris-setosa\n...\n```\n**通过名称列表选择多个列数据**\n```python\nprint(iris[['sepal_length', 'species']].peek())\n```\n*输出:*\n```bash\n    sepal_length      species\n0            5.1  Iris-setosa\n1            4.9  Iris-setosa\n2            4.7  Iris-setosa\n3            4.6  Iris-setosa\n4            5.0  Iris-setosa\n5            5.4  Iris-setosa\n...\n```\n#### 2. 数学操作\n**使用数学的操作和\b功能**\n```python\nfrom blaze import log\nprint(log(iris.sepal_length * 10).peek())\n```\n*数学功能像log应该从blaze导入,\b基于后端它会被转化为`np.log`、`math.log`、`sqlalchemy.sql.func.log`等.*\n\n#### 3. Reductions\n**与许多blaze操作一样,像`mean`和`sum`可以作为函数或基本功能使用.**\n```python\nprint(iris.sepal_length.mean().peek())\n# output: 5.843333333333334\nprint(mean(iris.sepal_length).peek())\n# output: 5.843333333333334\n```\n#### 4. Split-Apply-Combine\n*`by`操作是split-apply-combine计算,常见格式如下:*\n```bash\nby(table.grouping_columns, name_1=table.column.reduction(), name_2=table.column.reduction(), ...)  \n```\n*例如:根据species查找最短、\b最长和p平均的petal长度.*\n```python\nprint(by(iris.species, shortest=iris.petal_length.min(), longest=iris.petal_length.max(),\n         average=iris.petal_length.mean()).peek())\n```\n*输出:*\n```bash\n           species  average  longest  shortest\n0      Iris-setosa    1.462      1.9       1.0\n1  Iris-versicolor    4.260      5.1       3.0\n2   Iris-virginica    5.552      6.9       4.5\n```\n#### 5. \b添加新列\n**使用`transform`方法添加新列.**\n```python\nfrom blaze import transform\nprint(transform(iris, sepal_ratio = iris.sepal_length / iris.sepal_width,\n                petal_ratio = iris.petal_length / iris.petal_width).peek())\n```\n*输出:*\n```bash\n    sepal_length  sepal_width  petal_length  petal_width      species  petal_ratio  sepal_ratio\n0            5.1          3.5           1.4          0.2  Iris-setosa     7.000000     1.457143\n1            4.9          3.0           1.4          0.2  Iris-setosa     7.000000     1.633333\n2            4.7          3.2           1.3          0.2  Iris-setosa     6.500000     1.468750\n...\n```\n#### 6. 文本匹配\n```python\nprint(iris[iris.species.like('*versicolor')].peek())\n```\n*输出:*\n```bash\n    sepal_length  sepal_width  petal_length  petal_width          species\n50           7.0          3.2           4.7          1.4  Iris-versicolor\n51           6.4          3.2           4.5          1.5  Iris-versicolor\n...\n```\n#### 7. 重命名列名\n```python\nprint(iris.relabel(petal_length='PETAL-LENGTH', petal_width='PETAL-WIDTH').peek())\n```\n*输出:*\n```bash\n    sepal_length  sepal_width  PETAL-LENGTH  PETAL-WIDTH      species\n0            5.1          3.5           1.4          0.2  Iris-setosa\n1            4.9          3.0           1.4          0.2  Iris-setosa\n...\n```\n\n#### 8. 例子\n*Blaze可以解决许多数据分析和科学计算\b中存在的问题,例子如下:*\n\n* Combining separate, gzipped csv files\n  ```python\n  # coding: utf-8\n\n  from blaze import odo\n  from blaze.utils import example\n  from pandas import DataFrame\n\n  print(odo(example('accounts_*.csv.gz'), DataFrame))\n  ```\n  *输出:*\n  ```bash\n    id      name  amount\n    0   1     Alice     100\n    1   2       Bob     200\n    2   3   Charlie     300\n    3   4       Dan     400\n    4   5     Edith     500\n\n  ```\n* Split-Apply-Combine\n  ```python\n  from blaze import by\n  t = data('sqlite:///%s::iris' % example('iris.db'))\n  print(by(t.species, max=t.petal_length.max(), min=t.petal_length.min()).peek())\n  ```\n  *输出:*\n  ```bash\n    species  max  min\n    0      Iris-setosa  1.9  1.0\n    1  Iris-versicolor  5.1  3.0\n    2   Iris-virginica  6.9  4.5\n  ```\n ","tags":["Python","Blaze"],"categories":["Blaze"]},{"title":"Blaze(三):快速入门","url":"/2018/12/04/blaze/quickstart/","content":"\n\n*该篇文章通过展示创建和操作Blaze Symbols快速入门。*\n\n#### 1. 与数据的交互\n\n*通过嵌套的list/tuple创建简单的Blaze表达式。Blaze将推导出要使用的维度和数据类型。*\n\n```python\n# coding: utf-8\n\nfrom blaze import *\n\nt = data([(1, 'Alice', 100),\n          (2, 'Bob', -200),\n          (3, 'Charlie', 300),\n          (4, 'Denis', 400),\n          (5, 'Edith', -500)],\n         fields=['id', 'name', 'balance'])\n\nprint(t.peek())\n```\n\n*输出：*\n\n```bash\n   id     name  balance\n0   1    Alice      100\n1   2      Bob     -200\n2   3  Charlie      300\n3   4    Denis      400\n4   5    Edith     -500\n```\n>  *可能报错：* AttributeError: 'Graph' object has no attribute 'edges_iter'\n>\n> 解决方法：安装networkx的1.9版本。\n\n#### 2. 简单计算\n\n*与Pandas相似的列选择和过滤语法*\n\n```python\n# coding: utf-8\nfrom blaze import *\n\nt = data([(1, 'Alice', 100),\n          (2, 'Bob', -200),\n          (3, 'Charlie', 300),\n          (4, 'Denis', 400),\n          (5, 'Edith', -500)],\n         fields=['id', 'name', 'balance'])\n\nprint(t[t.balance < 0].peek())\nprint('-' * 10)\nprint(t[t.balance < 0].peek().name)\n```\n> 在0.11版本中Blaze表达式\brepr不再隐式地\b计算,需要使用peek()函数去看改表达式结果的预览.\n\n*输出:*\n```bash\n   id   name  balance\n0   2    Bob     -200\n1   5  Edith     -500\n----------\n0      Bob\n1    Edith\n\n```\n\n#### 3. 已保存的数据\n*操作[iris](https://raw.githubusercontent.com/blaze/blaze/master/blaze/examples/data/iris.csv)数据集的CSV文件*\n```python\n# coding: utf-8\n\nfrom blaze import *\nfrom blaze.utils import example\n\niris = data(example('iris.csv'))\nprint(iris.peek())\n```\n*输出:*\n```bash\n    sepal_length  sepal_width  petal_length  petal_width      species\n0            5.1          3.5           1.4          0.2  Iris-setosa\n1            4.9          3.0           1.4          0.2  Iris-setosa\n2            4.7          3.2           1.3          0.2  Iris-setosa\n3            4.6          3.1           1.5          0.2  Iris-setosa\n4            5.0          3.6           1.4          0.2  Iris-setosa\n5            5.4          3.9           1.7          0.4  Iris-setosa\n6            4.6          3.4           1.4          0.3  Iris-setosa\n7            5.0          3.4           1.5          0.2  Iris-setosa\n8            4.4          2.9           1.4          0.2  Iris-setosa\n9            4.9          3.1           1.5          0.1  Iris-setosa\n10           5.4          3.7           1.5          0.2  Iris-setosa\n```\n*用相同的方式使用远程数据,例如:SQL数据库或者spark分布式数据结构.例子: 操作sqlite中的数据库.*\n```python\n    sepal_length  sepal_width  petal_length  petal_width      species\n0            5.1          3.5           1.4          0.2  Iris-setosa\n1            4.9          3.0           1.4          0.2  Iris-setosa\n2            4.7          3.2           1.3          0.2  Iris-setosa\n3            4.6          3.1           1.5          0.2  Iris-setosa\n4            5.0          3.6           1.4          0.2  Iris-setosa\n5            5.4          3.9           1.7          0.4  Iris-setosa\n6            4.6          3.4           1.4          0.3  Iris-setosa\n7            5.0          3.4           1.5          0.2  Iris-setosa\n8            4.4          2.9           1.4          0.2  Iris-setosa\n9            4.9          3.1           1.5          0.1  Iris-setosa\n10           5.4          3.7           1.5          0.2  Iris-setosa\n```\n#### 4. 更多计算\n*常用操作例如Joins和split-apply-combine对任何种类的数据都可用.*\n```python\n# coding: utf-8\n\nfrom blaze import *\nfrom blaze.utils import example\n\niris = data('sqlite:///{}::iris'.format(example('iris.db')))\nprint(by(iris.species, min=iris.petal_width.min(), max=iris.petal_width.max()).peek())\n```\n*输出:*\n```bash\n           species  max  min\n0      Iris-setosa  0.6  0.1\n1  Iris-versicolor  1.8  1.0\n2   Iris-virginica  2.5  1.4\n```\n#### 5. 结束\n*使用`odo`操作将结果输出到一个合适的容器类型中.*\n```python\n# coding: utf-8\n\nfrom blaze import *\nfrom blaze.utils import example\n\niris = data('sqlite:///{}::iris'.format(example('iris.db')))\nresult = by(iris.species, avg=iris.petal_width.mean())\nresult_list = odo(result, list)\nprint(odo(result, DataFrame))\n```\n*输出:*\n```bash\n           species    avg\n0      Iris-setosa  0.246\n1  Iris-versicolor  1.326\n2   Iris-virginica  2.026\n```","tags":["Python","Blaze"],"categories":["Blaze"]},{"title":"Blaze(二):安装","url":"/2018/12/04/blaze/install/","content":"\n\n* **conda方式**\n\n  ```bash\n  conda install blaze\n  # 更多最新的构建\n  conda install -c blaze blaze\n  ```\n\n* **pip方式**\n\n  ```bash\n  pip install blaze --upgrade\n  or\n  pip install git+https://github.com/blaze/blaze  --upgrade\n  ```\n\n* **源码方式**\n\n  ```bash\n  git clone git@github.com:blaze/blaze.git\n  cd blaze\n  python setup.py install\n  ```\n\n**必要依赖：**\n\n- [numpy](http://www.numpy.org/) >= 1.7\n- [datashape](https://github.com/blaze/datashape) >= 0.4.4\n- [odo](https://github.com/blaze/odo) >= 0.3.1\n- [toolz](http://toolz.readthedocs.org/) >= 0.7.0\n- [cytoolz](https://github.com/pytoolz/cytoolz/)\n- [multipledispatch](http://multiple-dispatch.readthedocs.org/) >= 0.4.7\n- [pandas](http://pandas.pydata.org/)\n\n**可选依赖：**\n\n- [sqlalchemy](http://www.sqlalchemy.org/)\n- [h5py](http://docs.h5py.org/en/latest/)\n- [spark](http://spark.apache.org/) >= 1.1.0\n- [pymongo](http://api.mongodb.org/python/current/)\n- [pytables](http://www.pytables.org/moin)\n- [bcolz](https://github.com/Blosc/bcolz)\n- [flask](http://flask.pocoo.org/) >= 0.10.1\n- [pytest](http://pytest.org/latest/) (for running tests)","tags":["Python","Blaze"],"categories":["Blaze"]},{"title":"Blaze(一):前言","url":"/2018/12/04/blaze/overview/","content":"\n\n*Blaze生态系统为python用户对大数据提供了高效计算的高层接口。主要由Anaconda赞助。*\n\n**应用领域**\n\n*Blaze整合了包括Python的Pandas、NumPy及SQL、Mongo、Spark在内的多种技术，使用Blaze能够非常容易地与一个新技术进行交互。*\n\n*Blaze目前主要用于数据库和数组技术的分析查询。并且它在不断地整合和提供基于其它计算系统的应用接口。Blaze主要通过为数据科学家提供直观的各种工具访问来展现性能。*\n\n**Blaze生态系统：**\n\n* Blaze项目：忽略用户使用的各种不同的计算方案和不同类型的数据库，为用户提供了统一、友好、熟悉的界面。它帮助我们更好地与文档、数据结构以及数据库进行交互，根据需要适当优化和转换用户查询语句以提供一个流畅和交互式会话。Blaze项目允许数据科学家和分析人员以统一的方式编写他们的查询语句，不必因为数据存储格式或存储介质的不同而进行转换。它还提供了一个服务器组件，允许使用URIs来方便地提供数据视图，并在本地脚本、查询和程序中远程引用数据。\n\n  将NumPy/Pandas式的句法规则转换为数据计算系统（例如数据库、内存、分布式计算）。这为Python用户查询存储于其他数据存储系统中的数据提供了便捷和熟悉的界面。一条Blaze查询命令能够完成从CSV文件到分布式数据库的各种数据处理。\n\n* DataShape：数据类型系统\n\n  结合了NumPy中的dtype和数据形状，并且将内容扩展到了缺失值、可变长度字符串、不规则数组以及多任意嵌套方面。它包含了从数据库到HDF5文件再到JSON片段数据类型的统一描述。\n\n* Odo：实现不同格式数据的转换\n\n  利用一个复杂可扩展的转换网络，通过一个简单的界面有效稳定地实现了不同类型（CSV、JSON、数据库）和不同位置（本地、远程、HDFS）数据的传输和转换。\n\n* DyND：内存中的动态数组\n\n  DyND是一个类似于NumPy一样的动态ND数组库，是一个实现数据格式类型处理的系统。它支持可变长度的字符串、不规则数据以及GPU。它是一个与Python绑定的独立的C++代码库。通常DyND比NumPy更具扩展性，但不如NumPy成熟。\n\n* Dask.array：多核/基于磁盘存储的Numpy数组\n\n  Dask.dataframe：多核/基于磁盘的Pandas数据帧\n\n  Dask.arrays在NumPy之上提供了阻塞算法，并利用多核处理大于内存的数组。它们是NumPy算法常用集的简单替换。\n\n  Dask.dataframes在Pandas之上提供了阻塞算法，并利用多核处理大于内存的数据帧（dataframe），它们是Pandas用例算法集的简单替换。\n\n  除了多核和多线程调度器外，Dask还利用简单装饰器和新生分布式调度器为用户提供了一个“Bag”类型数据和一种构建“任务图（task graphs）”的方法。\n\n> 以上部分来自于博客[http://hao.jobbole.com/blaze/](http://hao.jobbole.com/blaze/)\n\n\n\n**例子**\n\n*Blaze将要执行的计算从数据的描述中分离*\n\n```python\n# coding: utf-8\n\nfrom blaze import *\n\n\naccounts = symbol('accounts', 'var * {id: int, name: string, amount: int}')\n\ndeadbeats = accounts[accounts.amount < 0].name\n\nL = [[1, 'Alice',   100],\n     [2, 'Bob',    -200],\n     [3, 'Charlie', 300],\n     [4, 'Denis',   400],\n     [5, 'Edith',  -500]]\n\nprint(list(compute(deadbeats, L)))\n\ndf = DataFrame(L, columns=['id', 'name', 'amount'])\nprint(compute(deadbeats, df))\n\n```\n\n*输出：*\n\n```bash\n['Bob', 'Edith']\n1      Bob\n4    Edith\nName: name, dtype: object\n```\n\n*Blaze不计算这些结果，它聪明地驱动其他项目来计算。这些项目从简单的纯python迭代器到高性能、分布式的spark集群。*","tags":["Python","Blaze"],"categories":["Blaze"]},{"title":"使用Psycopg2高效更新数据(二)","url":"/2018/12/03/python/python_psycopg_2/","content":"\n\n\n> python: 3.7\n>\n> pscopg2: 2.7\n\n\n\n参考文档[Server side cursors](http://initd.org/psycopg/docs/usage.html#server-side-cursors)\n\n*当执行一个数据库查询时，Pscopg cursor通常将查询到的所有数据返回给客户端，如果返回的数据过大，则将占用客户端大量的内存。因此，psycopg提供了一种成为`server side curosr`机制，每次返回可控制数量的数据。*\n\n*Server side cursor是使用PostgreSQL的`DECLARE`命令创建，并经过`MOVE`、`FETCH`和`CLOSE`命令处理的。*\n\n*Psycopg通过命名的cursors装饰server side cursor的，而命名cursor是通过对`cursor()`方法指定`name`参数而创建的。server side cursor允许用户在数据集中使用`scroll()`移动游标，并通过`fetchone()`和`fetchmany()`方法获取数据。*\n\n* scrollable：控制游标是否可以向后移动\n* itersize：控制每次可以获取多少条数据，默认是2000\n\n\n\n*当命名cursor尝试在`commit()`方法执行后获取数据或者由一个`autocommit\t`模式的connection创建命名cursor时都将导致一个错误。*\n\n*使用示例：fetchone*\n\n```python\nimport time\nimport datetime\nfrom dateutil import tz\n\nfrom psycopg2 import connect\nfrom psycopg2.extras import RealDictCursor, execute_values\n\n\nDB_HOST = 'localhost'\nDB_PORT = 5432\nDB_USERNAME = 'root'\nDB_PASSWORD = '123456'\nDB_NAME = 'test'\n\n\ndef date_range():\n    now = datetime.datetime.utcnow()\n    start_at = datetime.datetime(2018, 1, 1, 0, 0, 0, tzinfo=tz.gettz(\"utc\"))\n    end_at = now - datetime.timedelta(days=180)\n\n    data = {\n            0: [start_at, end_at]\n            }\n    return data\n\n\ndef connection():\n    with connect(database=DB_NAME,\n                 host=DB_HOST,\n                 port=DB_PORT,\n                 user=DB_USERNAME,\n                 password=DB_PASSWORD) as conn:\n        with conn.cursor(name='server_cursor', cursor_factory=RealDictCursor) as cur:\n            data = date_range()\n            for k, v in data.items():\n                sql = \"SELECT id, score FROM review \" \\\n                      \"WHERE create_date >= %s AND create_date < %s AND score IS NULL;\"\n                cur.execute(sql, (v[0], v[1]))\n                \n                for c in cur:\n                    print(c)\n\n\nif __name__ == '__main__':\n    now = time.time()\n    connection()\n    print('time is %d' % (time.time() - now))\n```\n\n*使用示例：fetchmany*\n\n```python\nimport time\nimport datetime\nfrom dateutil import tz\n\nfrom psycopg2 import connect\nfrom psycopg2.extras import RealDictCursor, execute_values\n\n\nDB_HOST = 'localhost'\nDB_PORT = 5432\nDB_USERNAME = 'root'\nDB_PASSWORD = '123456'\nDB_NAME = 'test'\n\n\ndef date_range():\n    now = datetime.datetime.utcnow()\n    start_at = datetime.datetime(2018, 1, 1, 0, 0, 0, tzinfo=tz.gettz(\"utc\"))\n    end_at = now - datetime.timedelta(days=180)\n\n    data = {\n            0: [start_at, end_at]\n            }\n    return data\n\n\ndef connection():\n    with connect(database=DB_NAME,\n                 host=DB_HOST,\n                 port=DB_PORT,\n                 user=DB_USERNAME,\n                 password=DB_PASSWORD) as conn:\n        with conn.cursor(name='server_cursor', cursor_factory=RealDictCursor) as cur:\n            data = date_range()\n            for k, v in data.items():\n                sql = \"SELECT id, score FROM review \" \\\n                      \"WHERE create_date >= %s AND create_date < %s AND score IS NULL;\"\n                cur.execute(sql, (v[0], v[1]))\n\n                while True:\n                    rows = cur.fetchmany(2000)\n                    if len(rows) > 0:\n                        values = []\n                        for row in rows:\n                            score = 5\n                            values.append((row['id'], score))\n                        sql = \"UPDATE review SET score=data.score FROM (VALUES %s) AS data (id, score) \" \\\n                              \"WHERE review.id = data.id;\"\n                        execute_values(cur, sql, values, page_size=100)\n                    else:\n                        break\n\n\nif __name__ == '__main__':\n    now = time.time()\n    connection()\n    print('time is %d' % (time.time() - now))\n```\n\n","tags":["Python","Psycopg2"],"categories":["python"]},{"title":"使用Psycopg2高效更新数据(一)","url":"/2018/11/30/python/python_psycopg_1/","content":"\n\n> Python: 3.7\n>\n> Psycopg: 2.7\n\n\n\n> 最近要对Postgresql数据库某表中的几百万条数据进行计算并更新某字段的值，在此期间使用过协程+aiopg，7分钟更新2000条数据，速度太慢；后来查看Psycopg2文档发现了一个高效的方法。\n\n\n\n**安装Psycopg **\n\n```bash\npip install psycopg2\n```\n\n**文档中关于高效执行的描述：[Fast execution helpers](http://initd.org/psycopg/docs/extras.html#fast-exec).**\n\n\n\n**psycopg2.extras.execute_values(*cur*, *sql*, *argslist*, *template=None*, *page_size=100*)**\n\n*使用该方法可以快速批量更新，其中函数中的page_size参数默认为100，表示每个statement包含的最大条目数，如果传过来的argslist长度大于page_size,则该函数最多执行len(argslist)/page_size + 1次。*\n\n*示例程序*\n\n```python\n# coding: utf-8\n\nimport time\nimport datetime\n\nfrom psycopg2 import connect\nfrom psycopg2.extras import RealDictCursor, execute_values\n\n\nDB_HOST = 'localhost'\nDB_PORT = 5432\nDB_USERNAME = 'root'\nDB_PASSWORD = '123456'\nDB_NAME = 'test'\n\n\ndef date_range():\n    now = datetime.datetime.utcnow()\n    start_at = datetime.datetime(2018, 1, 1, 0, 0, 0, tzinfo=tz.gettz(\"utc\"))\n    end_at = now - datetime.timedelta(days=180)\n\n    data = {\n            0: [start_at, end_at]\n            }\n    return data\n\n\ndef connection():\n    with connect(database=DB_NAME,\n                 host=DB_HOST,\n                 port=DB_PORT,\n                 user=DB_USERNAME,\n                 password=DB_PASSWORD) as conn:\n        with conn.cursor(cursor_factory=RealDictCursor) as cur:\n            data = date_range()\n            for k, v in data.items():\n                for i in range(50):\n                    sql = \"SELECT id, score FROM review \" \\\n                          \"WHERE create_date >= %s AND create_date < %s AND score IS NULL limit 2000;\" \n                    cur.execute(sql, (v[0], v[1]))\n\n                    values = []\n                    for row in cur:\n                        score = 5 \n                        values.append((row['id'], score))\n                        sql = \"UPDATE review SET score=data.score FROM (VALUES %s) AS data (id, score) \" \\\n                              \"WHERE review.id = data.id;\"\n                    execute_values(cur, sql, values, page_size=100)\n                    print(cur.query)\n\n\nif __name__ == '__main__':\n    now = time.time()\n    connection()\n    print('time is %d' % (time.time() - now))\n\n\n```\n\n","tags":["Python","Psycopg2"],"categories":["python"]},{"title":"Elasticsearch(十一)利用logstash将mysql数据输出到ES","url":"/2018/11/29/es_logstash_jdbc/","content":"\n> Elasticsearch: 6.4.2\n> Logstash: 6.4.2\n\n\n**利用Logstash插件logstash-input-jdbc将mysql数据库中的数据存到ES中.**\n\n**准备工作:**\n* [下载logstash](https://www.elastic.co/downloads/past-releases/logstash-6-4-2)\n* [下载mysql jdbc jar包](https://dev.mysql.com/downloads/connector/j/)\n\n*logstatsh的pipeline文件：*\n```yaml\n# Sample Logstash configuration for creating a simple\n# Beats -> Logstash -> Elasticsearch pipeline.\n\ninput {\n    stdin {\n    }\n    jdbc {\n      jdbc_connection_string => \"jdbc:mysql://127.0.0.1:3306/test?serverTimezone=Asia/Shanghai&useSSL=true&useUnicode=true&characterEncoding=UTF-8\"\n      jdbc_user => \"root\"\n      jdbc_password => \"123456\"\n      jdbc_driver_library => \"../jar/mysql-connector-java-5.1.47-bin.jar\"\n      jdbc_driver_class => \"com.mysql.jdbc.Driver\"\n      jdbc_paging_enabled => \"true\"\n      jdbc_page_size => \"50000\"\n      statement => \"select * from user\"\n      type => \"jdbc\"\n    }\n}\nfilter {\n    json {\n        source => \"message\"\n        remove_field => [\"message\"]\n    }\n}\noutput {\n    elasticsearch {\n        hosts => [\"127.0.0.1:9200\"]\n        index => \"users\"\n        document_id => \"%{id}\"\n    }\n    stdout {\n        codec => json_lines\n    }\n}\n```\n* jdbc_driver_library：路径是相对于logstash根目录下的config目录。\n\n*启动*\n```bash\nbin/logstash -f ../config/logstash-mysql.conf\n```","tags":["Elasticsearch","Logstash"],"categories":["大数据"]},{"title":"Elasticsearch(十) Logstash入门","url":"/2018/11/28/es_logstash/","content":"\n> Logstash: 6.4.2\n\n### 1. 配置语法\n\n#### 1.1 语法\n\n**区段**\nLogstash使用`{}`来定义区段,区域内可以包含插件区域定义,可以在一个区域内定义多个插件;插件区域内可以定义键值设置,例如:\n```bash\ninput {\n    stdin{}\n}\n```\n\n**数据类型**\n* bool\n* string\n* number\n* array\n* hash\n\n**字段引用**\n*字段是`Logstash::Event`对象的属性,将字段的名称写在`[]`里就可以在Logstash配置里使用该字段的值.*\n*对于嵌套字段,可以使用如下格式:*\n```bash\n[字段名][字段名或数组下标值]\n# 例如:\n[user][firstname]\n[users][0]\n```\n> Logstash里数组支持倒序下标,例如:[-1]表示数组的最后一个元素.\n*Logstash还支持变量内插,在字符串里使用字段引用的方法如下:*\n```bash\n\"the first name is %{[user][firstname]}\"\n```\n**条件判断**\n* ==(等于)、!=(不等于)、<(小于)、<=(小于等于)、 >(大于)、>=(大于等于)\n* =~(正则匹配)、!~(不正则匹配)\n* in(包含)、not in(不包含)\n* and(与)、or(或)、nand(非与)、xor(非或)\n* ()(复合表达式)、!()(对复合表达式结果取反)\n\n#### 1.2 命令行参数解析\n* -e\n  意即执行,默认值为:\n  ```ruby\n  input {\n      stdin{ }\n  }\n  output {\n      stdout{ }\n  }\n  ```\n\n* --config或-f\n  指定配置文件.\n* --configtest或-t\n  用来测试Logstash读取到的配置文件语法是否能正常解析.\n* --log或-l\n  指定日志输出位置,默认输出日志到标准错误.\n* --pipeline-workers或-w\n  指定运行filter和output的pipeline线程数量,默认为CPU核数.\n* --pipeline-batch-size或-b\n  指定每个pipeline线程在打包批量日志的时候最多等待几秒,默认为5ms.\n* --pluginpath或-P\n  指定自定义插件的路径\n* --verbose\n  输出一定的调试日志\n* --debug\n  输出更多的调试日志\n\n#### 1.3 配置文件\n*针对命令行参数,我们可以在`$LS_HOME/config/logstash.yml`文件中进行设置,例如:*\n```yaml\npipeline:\n    workers: 24\n    batch:\n        size: 125\n        delay: 5\n```\n\n### 2. 执行过程\n\n**Logstash事件进程管道有三个阶段：input->filter->output，其中input和output是必须的，filter是可选的；input默认为stdin，output默认为stdout。**\n\n* inputs：常用的有file、syslog、redis、jdbc、beats等。\n* filters：常用的有：\n  * grok：解析和结构化任意文本。常用来解析非结构化的log；内置120种解析模式。\n  * mutate：对事件类型执行转换，可以rename、remove、replace、modify。\n  * drop：完全丢掉一种事件。\n  * geoip：添加IP地址的地理位置信息。\n\n* outputs：常用的有elasticsearch、file、graphite、statsd。\n\n**Codecs是基本的对input/output的流式过滤器，常见的有json、multiline、plain。**\n\n> 执行过程：logstash管道中每个input阶段都运行在独立的线程中。Inputs将事件写到一个存在于内存或硬盘的集中式queue中；每个管道worker从queue取一批事件（数量可配置）并按序执行配置的filters，最后将结果写入到outputs。\n\n\n\n### 3. 简单实例\n\n下载filebeat、logstash二进制包并解压。\n\n下载[apache日志](https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz)。\n\n**filebeat读取apache日志并发送到logstash，logstash将其输出到stdout。**\n\n*filebeat.yml*\n\n```yaml\nfilebeat.prospectors:\n- type: log\n  paths:\n    - /path/to/logstash-tutorial.log \noutput.logstash:\n  hosts: [\"localhost:5044\"]\n```\n\n*启动filebeat*\n\n```bash\n./filebeat -e -c filebeat.yml -d \"publish\"\n```\n\n*logstash-sample.conf*\n\n```conf\ninput {\n  beats {\n        port => \"5044\"\n    }\n}\n\noutput {\n  stdout { codec => rubydebug }\n}\n```\n\n*启动logstash*\n\n```bash\nbin/logstash -f config/logstash-sample.conf --config.reload.automatic\n```\n\n* --config.reload.automatic：自动重加载配置文件，当你修改配置文件后不再需要stop并restart logstash了。","tags":["Elasticsearch、Logstash"],"categories":["大数据"]},{"title":"Elasticsearch(八)搜索优化","url":"/2018/11/22/es_optimize/","content":"\n> Elasticsearch: 6.4.2\n#### 1. 理解字段分析过程\n\n*一个常被问到的问题是，为什么指定的文档没有被搜索到。很多情况下，这都归因于映射的定义和分析例程的配置存在问题。针对分析过程的调试，Elasticsearch提供了专用的REST API。*\n\n```bash\nGET /_analyze\n{\n  \"analyzer\": \"standard\",   # 可以替换成自定义的analyzer\n  \"text\": \"crime and publishment\"\n}\n# 使用一个分词器和两个过滤器 \nGET _analyze\n{\n  \"tokenizer\" : \"keyword\",\n  \"filter\" : [\"lowercase\"],\n  \"char_filter\" : [\"html_strip\"],\n  \"text\" : \"this is a <b>test</b>\"\n}\n```\n\n#### 2. 解释查询\n\n*给出有关文档相关度得分计算的详细信息，以解释为什么会匹配成功。*\n\n```bash\n# 针对特定文档的分析\nGET /cars/doc/8/_explain?q=jeep\n# 针对文本信息的分析\nGET _analyze\n{\n  \"tokenizer\" : \"standard\",\n  \"filter\" : [\"snowball\"],\n  \"text\" : \"detailed output\",\n  \"explain\" : true,\n  \"attributes\" : [\"keyword\"] \n}\n```\n\n#### 3. 用加权查询影响得分\n\nBoost(权值)：在计算得分过程中使用的附加权值，可在如下位置使用：\n\n* 查询：告诉搜索引擎指定的查询比其他查询更重要\n* 字段：指定文档中某些字段对用户很重要\n* 文档：某些文档很重要\n\n##### 3.1 在查询中使用权值\n\n*例如：*\n\n```bash\n# 字段重要程度：from、to、subject\nGET /emails/doc/_search\n{\n  \"query\": {\n    \"query_string\": {\n      \"fields\": [\"from^5\", \"to^3\", \"subject\"], \n      \"query\": \"Bourne\"\n    }\n  }\n}\n```\n\n##### 3.2 修改得分\n\n**constant_score**\n\n*包装另一个查询子句，为每个文档返回得分等于boost值。*\n\n```bash\nGET /cars/doc/_search\n{\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"match\": {\n          \"name\": \"bmw jeep\"\n        }\n      },\n      \"boost\": 1.2\n    }\n  }\n}\n```\n\n**Boosting查询**\n\n*Boosting查询与Bool查询中的NOT的不同：后者过滤掉满足NOT查询语句的文档，而前者仍然选择不被期望的文档，只是讲它们的得分降低。*\n\n```bash\nGET /cars/doc/_search\n{\n  \"query\": {\n    \"boosting\": {\n      \"positive\": {\n        \"match_all\": {}\n      },\n      \"negative\": {\n        \"term\": {\n          \"name\": {\n            \"value\": \"jeep\"\n          }\n        }\n      },\n      \"negative_boost\": 0.2\n    }\n  }\n}\n```\n\n**Function查询**\n\n*boost参数设置每个文档的得分，然后对于满足functions参数中子查询条件的文档根据boost_mode、boost、weight计算新得分；*\n\n*当存在多个function且某个文档满足多个function时该文档的得分为：先计算满足各个function后的每个得分，然后由score_mode参数计算最终等分。其中score_mode取值：multiply、sum、avg、first、max、min。*\n\n```bash\nGET /cars/doc/_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"boost\": 5,\n      \"max_boost\": 50,\n      \"score_mode\": \"max\",\n      \"boost_mode\": \"multiply\",\n      \"query\": {\n        \"match_all\": {}\n      },\n      \"functions\": [\n        {\n          \"filter\": {\n            \"match\": {\"name\": \"jeep\"}\n          },\n          \"weight\": 4\n        }\n      ]\n    }\n  }\n}\n```\n\n#### 4. 具有相同含义的词\n\n##### 4.1 同义词(synonym)过滤器\n\n*基于数组定义*\n\n```bash\nPUT /test_index\n{\n    \"settings\": {\n        \"index\" : {\n            \"analysis\" : {\n                \"analyzer\" : {\n                    \"synonym\" : {\n                        \"tokenizer\" : \"standard\",\n                        \"filter\" : [\"my_stop\", \"synonym\"]\n                    }\n                },\n                \"filter\" : {\n                    \"synonym\" : {\n                        \"type\" : \"synonym\",\n                        \"lenient\": true,\n                        \"synonyms\" : [\"foo, bar => baz\"]\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n*基于文件定义：文件路径是相对于Elasticsearch安装目录下config目录的。*\n\n```bash\nPUT /test_index\n{\n    \"settings\": {\n        \"index\" : {\n            \"analysis\" : {\n                \"analyzer\" : {\n                    \"synonym\" : {\n                        \"tokenizer\" : \"whitespace\",\n                        \"filter\" : [\"synonym\"]\n                    }\n                },\n                \"filter\" : {\n                    \"synonym\" : {\n                        \"type\" : \"synonym\",\n                        \"synonyms_path\" : \"analysis/synonym.txt\"\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n##### 4.2 同义词定义规则\n\n*默认使用Apache solr的同义词方案。*\n\n**同义词显式定义**\n\n```bash\ni-pod, i pod => ipod,\nsea biscuit, sea biscit => seabiscuit\n```\n\n**同义词等式定义**\n\n```bash\nipod, i-pod, i pod\nfoozball , foosball\nuniverse , cosmos\nlol, laughing out loud\n```\n\n**扩展同义词**\n\n*如果同义词过滤器中属性expand=true，则所有同义词被扩展为所有单词全部等价的形式。*\n\n```bash\nipod, i-pod, i pod => ipod, i-pod, i pod\n```\n\n#### 5. 跨度查询\n\n*跨度是指在一个字段中开始和结束的词条位置。*\n\n**span_term**\n\n```bash\nGET /_search\n{\n    \"query\": {\n        \"span_term\" : { \"user\" : \"kimchy\" }\n    }\n}\n```\n\n**span_multi**\n\n*包装一个term、range、prefix、wildcard、regexp或fuzzy查询。*\n\n```bash\nGET /_search\n{\n    \"query\": {\n        \"span_multi\":{\n            \"match\":{\n                \"prefix\" : { \"user\" :  { \"value\" : \"ki\" } }\n            }\n        }\n    }\n}\n```\n\n**span_first**\n\n*只允许返回在字段的前几个位置上匹配查询条件的文档。*\n\n```bash\n# 查询在user字段前三个位置出现kimchy的文档\nGET /_search\n{\n    \"query\": {\n        \"span_first\" : {\n            \"match\" : {\n                \"span_term\" : { \"user\" : \"kimchy\" }\n            },\n            \"end\" : 3\n        }\n    }\n}\n```\n\n**span_near**\n\n*可以在有多个其他跨度彼此接近时对文档进行搜索，该查询也是一个能将其他跨度查询包装起来的复合查询。*\n\n```bash\n# slop：控制在跨度之间允许的词项的数量\n# in_order: 限制匹配顺序；true时按照查询定义的顺序匹配文档\n# 该示例查询message字段包含world everyone的文档。\nGET /_search\n{\n    \"query\": {\n        \"span_near\" : {\n            \"clauses\" : [\n                { \"span_term\" : { \"message\" : \"world\" } },\n                { \"span_term\" : { \"message\" : \"everyone\" } }\n            ],\n            \"slop\" : 0,\n            \"in_order\" : true\n        }\n    }\n}\n```\n\n**span_or**\n\n```bash\n# 获取在message字段前两个位置处有world或者离everyone不超过一个位置处含有world的文档。\nGET /_search\n{\n  \"query\": {\n    \"span_near\": {\n      \"clauses\": [\n        {\n          \"span_first\": {\n            \"match\": {\n              \"span_term\": {\n                \"message\": {\n                  \"value\": \"world\"\n                }\n              }\n            },\n            \"end\": 2\n          }\n        },\n        {\n          \"span_near\": {\n            \"clauses\": [\n              {\n                \"span_term\": {\n                  \"message\": {\n                    \"value\": \"world\"\n                  }\n                }\n              },\n              {\n                \"span_term\": {\n                  \"message\": {\n                    \"value\": \"everyone\"\n                  }\n                }\n              }\n            ],\n            \"slop\": 1,\n            \"in_order\": true\n          }\n        }\n      ],\n      \"slop\": 0,\n      \"in_order\": true\n    }\n  }\n}\n```\n\n**span_not**\n\n*include参数指定了哪个跨度查询应该被匹配；exclude参数指定了不与include部分重叠的跨度查询。*\n\n```bash\n# 返回在message字段中匹配了由breaks词项构造的span_term查询的所有文档，然后再定义一个匹配了world和everyone并且最大位置间距为1的跨度，当该跨度与第一个跨度查询重叠时，排除掉所有重叠部分的文档。\nGET /_search\n{\n  \"query\": {\n    \"span_not\": {\n      \"include\": {\n        \"span_term\": {\n          \"message\": {\n            \"value\": \"breaks\"\n          }\n        }\n      },\n      \"exclude\": {\n        \"span_near\": {\n          \"clauses\": [\n            {\n              \"span_term\": {\n                \"message\": {\n                  \"value\": \"world\"\n                }\n              }\n            },\n            {\n              \"span_term\": {\n                \"message\": {\n                  \"value\": \"everyone\"\n                }\n              }\n            }\n          ],\n          \"slop\": 1\n        }\n      }\n    }\n  }\n}\n```\n\n**span_containing**\n\n*查询内部有big和little两个字查询，仅返回big匹配的结果中包含little匹配结果的文档；从大到小包含*\n\n```bash\nGET /_search\n{\n    \"query\": {\n        \"span_containing\" : {\n            \"little\" : {\n                \"span_term\" : { \"field1\" : \"foo\" }\n            },\n            \"big\" : {\n                \"span_near\" : {\n                    \"clauses\" : [\n                        { \"span_term\" : { \"field1\" : \"bar\" } },\n                        { \"span_term\" : { \"field1\" : \"baz\" } }\n                    ],\n                    \"slop\" : 5,\n                    \"in_order\" : true\n                }\n            }\n        }\n    }\n}\n```\n\n**span_within**\n\n*与span_containing类似，从小到大包含。*\n\n```bash\nGET /_search\n{\n    \"query\": {\n        \"span_within\" : {\n            \"little\" : {\n                \"span_term\" : { \"field1\" : \"foo\" }\n            },\n            \"big\" : {\n                \"span_near\" : {\n                    \"clauses\" : [\n                        { \"span_term\" : { \"field1\" : \"bar\" } },\n                        { \"span_term\" : { \"field1\" : \"baz\" } }\n                    ],\n                    \"slop\" : 5,\n                    \"in_order\" : true\n                }\n            }\n        }\n    }\n}\n```\n\n\n\n","tags":["Elasticsearch"],"categories":["大数据"]},{"title":"Elasticsearch(九)数据关联","url":"/2018/11/22/es_hierarchy/","content":"\n> Elasticsearch: 6.4.2 \n\n\n#### 1. 索引树形结构\n\n*创建简单映射*\n\n```bash\nPUT /categories\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"path_analyzer\": {\n          \"tokenizer\": \"path_hierarchy\"\n        }\n      }\n    }\n  },\n  \"mappings\": {\n    \"category\": {\n      \"properties\": {\n        \"content\": {\n          \"type\": \"text\",\n          \"fields\": {\n            \"path\": {\n              \"analyzer\": \"path_analyzer\",\n              \"type\": \"text\",\n              \"store\": true\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n*测试*\n\n```bash\nGET /categories/_analyze\n{\n  \"field\": \"content.path\",\n  \"text\": \"/home/ubuntu/work\"\n}\n# 输出：/home、/home/ubuntu、/home/ubuntu/work\n```\n\n#### 2. 嵌套对象\n\n*创建映射*\n\n```bash\nPUT my_index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"user\": {\n          \"type\": \"nested\" \n        }\n      }\n    }\n  }\n}\n```\n\n*索引数据*\n\n```bash\nPUT my_index/_doc/1\n{\n  \"group\" : \"fans\",\n  \"user\" : [\n    {\n      \"first\" : \"John\",\n      \"last\" :  \"Smith\"\n    },\n    {\n      \"first\" : \"Alice\",\n      \"last\" :  \"White\"\n    }\n  ]\n}\n```\n\n*搜索*\n\n```bash\nGET my_index/_search\n{\n  \"query\": {\n    \"nested\": {\n      \"path\": \"user\",\n      \"query\": {\n        \"bool\": {\n          \"must\": [\n            { \"match\": { \"user.first\": \"Alice\" }},\n            { \"match\": { \"user.last\":  \"white\" }} \n          ]\n        }\n      }\n    }\n  }\n}\n```\n\n#### 3. 父子关系\n\n* **6.0之后父子结构使用join类型来定义，关系的映射使用relations字段来指定。**\n* **一个索引中只能有一个join类型字段。**\n* **建好索引之后, join字段中的relations集合，可以增加映射、或者给原有的映射添加child，但是不能删除原有的映射。**\n* **父文档和子文档存在同一个分片上，因此查询、更新、删除子文档时应该使用routing参数。**\n\n*创建映射*\n\n```bash\nPUT my_index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"properties\": {\n        \"my_join_field\": { \n          \"type\": \"join\",\n          \"relations\": {\n            \"question\": \"answer\" \n          }\n        }\n      }\n    }\n  }\n}\n```\n\n*创建父文档*\n\n```bash\nPUT my_index/_doc/1?refresh\n{\n  \"text\": \"This is a question\",\n  \"my_join_field\": \"question\" \n}\n```\n\n*创建子文档*\n\n```bash\nPUT my_index/_doc/3?routing=1&refresh \n{\n  \"text\": \"This is an answer\",\n  \"my_join_field\": {\n    \"name\": \"answer\", \n    \"parent\": \"1\" \n  }\n}\n```\n\n*根据parent_id查询属于某一特定父文档的子文档*\n\n```bash\nGET /my_index/_search\n{\n  \"query\": {\n    \"parent_id\": {\n      \"type\": \"answer\",\n      \"id\": \"1\"\n    }\n  }\n}\n```\n\n*基于父文档查找子文档*\n\n```bash\nGET my_index/_search\n{\n    \"query\": {\n        \"has_parent\" : {\n            \"parent_type\" : \"question\",\n            \"query\" : {\n                \"match\" : {\n                    \"text\" : \"This is\"\n                }\n            }\n        }\n    }\n}\n```\n\n*基于子文档查找父文档*\n\n```bash\nGET my_index/_search\n{\n\"query\": {\n        \"has_child\" : {\n            \"type\" : \"answer\",\n            \"query\" : {\n                \"match\" : {\n                    \"text\" : \"This is question\"\n                }\n            }\n        }\n    }\n}\n```\n\n*Parent-join查询和聚合*\n\n* 查询父文档id=1，子文档类型为answer的子文档；\n* 基于父文档类型question进行聚合；\n* 基于指定的field处理。\n\n*如果一个文档是子文档，则`my_join_field#question`包含的是该子文档指向的父文档的id值；如果是父文档，则是该文档的id值。*\n\n```bash\nGET my_index/_search\n{\n  \"query\": {\n    \"parent_id\": { \n      \"type\": \"answer\",\n      \"id\": \"1\"\n    }\n  },\n  \"aggs\": {\n    \"parents\": {\n      \"terms\": {\n        \"field\": \"my_join_field#question\", \n        \"size\": 10\n      }\n    }\n  },\n  \"script_fields\": {\n    \"parent\": {\n      \"script\": {\n         \"source\": \"doc['my_join_field#question']\" \n      }\n    }\n  }\n}\n```\n\n","tags":["Elasticsearch"],"categories":["大数据"]},{"title":"Elasticsearch(七)信息检索与结果过滤","url":"/2018/11/15/es_aggregation/","content":"\n\n> Elasticsearch: 6.4.2\n\n\n**聚合分类：**\n\n*Bucketing聚合: 类似SQL中的GROUP BY；基于检索构成了逻辑文档组，满足特定规则的文档放置到一个桶里，每一个桶关联一个key; 分桶聚合可以嵌套分桶聚合。*\n\n*Metric聚合: 基于一组文档进行聚合。所有的文档在一个检索集合里，文档被分成逻辑的分组; 对一个数据集求最大、最小、和、平均值等指标的聚合。*\n\n*Matrix聚合： 此功能是实验性的，可在将来的版本中完全更改或删除；在多个字段上操作，并根据从请求的文档中提取的值生成矩阵结果。*\n\n*Pipeline聚合：对聚合的结果而不是原始数据集进行操作。*\n\n**测试数据**\n\n```json\nPOST userinfo/doc/_bulk\n{\n  \"index\": {\n    \"_id\": \"1\"\n  }\n}\n{\n  \"username\": \"alfred way\",\n  \"job\": \"java engineer\",\n  \"age\": 18,\n  \"birth\": \"1990-01-02\",\n  \"isMarried\": false,\n  \"salary\": 10000\n}\n{\n  \"index\": {\n    \"_id\": \"2\"\n  }\n}\n{\n  \"username\": \"tom\",\n  \"job\": \"java senior engineer\",\n  \"age\": 28,\n  \"birth\": \"1980-05-07\",\n  \"isMarried\": true,\n  \"salary\": 30000\n}\n{\n  \"index\": {\n    \"_id\": \"3\"\n  }\n}\n{\n  \"username\": \"lee\",\n  \"job\": \"ruby engineer\",\n  \"age\": 22,\n  \"birth\": \"1985-08-07\",\n  \"isMarried\": false,\n  \"salary\": 15000\n}\n{\n  \"index\": {\n    \"_id\": \"4\"\n  }\n}\n{\n  \"username\": \"Nick\",\n  \"job\": \"web engineer\",\n  \"age\": 23,\n  \"birth\": \"1989-08-07\",\n  \"isMarried\": false,\n  \"salary\": 8000\n}\n{\n  \"index\": {\n    \"_id\": \"5\"\n  }\n}\n{\n  \"username\": \"Niko\",\n  \"job\": \"web engineer\",\n  \"age\": 18,\n  \"birth\": \"1994-08-07\",\n  \"isMarried\": false,\n  \"salary\": 5000\n}\n{\n  \"index\": {\n    \"_id\": \"6\"\n  }\n}\n{\n  \"username\": \"Michell\",\n  \"job\": \"ruby engineer\",\n  \"age\": 26,\n  \"birth\": \"1987-08-07\",\n  \"isMarried\": false,\n  \"salary\": 12000\n}\n```\n\n* Metric聚合\n\n  **最值、求和、均值**\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n    \"size\": 0,\n    \"aggs\": {\n      \"avg_grade\": {\n        \"avg\": {  # 可以使用max、min、sum\n          \"field\": \"salary\"\n        }\n      }\n    }\n  }\n  ```\n\n  **Cardinality**\n\n  *类似于SQL中的district count*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n    \"size\": 0, \n    \"aggs\": {\n      \"type_count\": {\n        \"cardinality\": {\n          \"field\": \"job.keyword\"\n        }\n      }\n    }\n  }\n  ```\n\n  **Stats**\n\n  *返回一系列数值类型的统计值，包括min,max,avg,sum,count。*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"stats_age\":{\n              \"stats\":{\n                  \"field\":\"age\"\n              }\n          }\n      }\n  }\n  ```\n\n  **Extended Stats**\n\n  *对`Stats`聚合的扩展，包含更多统计数据，例如：方差，标准差等。*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"stats_age\":{\n              \"extended_stats\":{\n                  \"field\":\"age\"\n              }\n          }\n      }\n  }\n  ```\n\n  **Percentiles**\n\n  ```bash\n  # 百分位数统计\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"per_age\":{\n              \"percentiles\":{\n                  \"field\":\"salary\"\n              }\n          }\n      }\n  }\n  # 针对特定值计算百分位数\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"per_age\":{\n              \"percentile_ranks\":{\n                  \"field\":\"salary\",\n                  \"values\":[\n                      11000,\n                      30000\n                  ]\n              }\n          }\n      }\n  }\n  # 针对特定百分位数计算对应的值 \n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"per_age\":{\n              \"percentiles\":{\n                  \"field\":\"salary\",\n                  \"percents\" : [80, 95, 99, 99.9] \n              }\n          }\n      }\n  }\n  ```\n\n  **Top Hits**\n\n  *一般用于分桶后获取该桶内最匹配的顶部文档详情数据*\n\n  ```bash\n  # 先根据job分桶，后在桶内根据age排序\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"jobs\":{\n              \"terms\":{\n                  \"field\":\"job.keyword\",\n                  \"size\":10\n              },\n              \"aggs\":{\n                  \"top_employee\":{\n                      \"top_hits\":{\n                          \"size\":10,\n                          \"sort\":[\n                          {\n                              \"age\":{\n                                  \"order\":\"desc\"\n                              }\n                          }\n                          ]\n                      }\n                  }\n              }\n          }\n      }\n  }\n  ```\n\n* Bucketing聚合\n\n  **Terms**\n\n  *改分桶策略最简单，直接根据term分桶，如果是text类型，则按照分析器处理后的结果分桶。*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"jobs\":{\n              \"terms\":{\n                  \"field\":\"job.keyword\",\n                  \"size\":10\n              }\n          }\n      }\n  }\n  ```\n\n  **Range**\n\n  *通过指定数值的范围来设定分桶规则*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"salary_range\":{\n              \"range\":{\n                  \"field\":\"salary\",\n                  \"ranges\":[\n                  {\n                      \"to\":10000\n                  },\n                  {\n                      \"from\":10000,\n                      \"to\":20000\n                  },\n                  {\n                      \"from\":20000\n                  }\n                  ]\n              }\n          }\n      }\n  }\n  ```\n\n  **Date Range**\n\n  *通过指定日期的范围来设定分桶规则*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"date_range\":{\n              \"range\":{\n                  \"field\":\"birth\",\n                  \"format\":\"yyyy\",\n                  \"ranges\":[\n                  {\n                      \"from\":\"1980\",\n                      \"to\":\"1990\"\n                  },\n                  {\n                      \"from\":\"1990\",\n                      \"to\":\"2000\"\n                  },\n                  {   \n                      \"from\":\"2000\"\n                  }\n                  ]\n              }\n          }\n      }\n  }\n  ```\n\n  **Historgram**\n\n  *以固定间隔来分割数据*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"salary_hist\":{\n              \"histogram\":{\n                  \"field\":\"salary\",\n                   \"interval\":5000,\n                   \"extended_bounds\":{\n                      \"min\":0,\n                      \"max\":40000\n                   }\n              }\n          }\n      }\n  }\n  ```\n\n  **Date Historgram**\n\n  *针对日期的直方图*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"salary_hist\":{\n              \"date_histogram\":{\n                  \"field\":\"birth\",\n                   \"interval\":\"year\",\n                   \"format\":\"yyyy\"\n              }\n          }\n      }\n  }\n  ```\n\n  **Filter**\n\n  *将满足过滤条件的文档放入桶中*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n    \"size\": 0, \n    \"aggs\": {\n      \"salary_aggs\": {\n        \"filter\": {\n          \"range\": {\n            \"salary\": {\n              \"gte\": 10000\n            }\n          }\n        },\n        \"aggs\": {\n          \"avg_salary\": {\n            \"avg\": {\n              \"field\": \"salary\"\n            }\n          }\n        }\n      }\n    }\n  }\n  ```\n\n  **Missing**\n\n  *统计缺少指定字段的文档个数。*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n    \"size\": 0, \n    \"aggs\": {\n      \"miss_aggs\": {\n        \"missing\": {\n          \"field\": \"phone\"\n        }\n      }\n    }\n  }\n  ```\n\n* Pipeline聚合\n\n  **针对聚合分析的结果再次进行聚合分析，而且支持链式调用。**\n\n  *Pipeline的分析结果会输出到原结果中,根据输出位置的不同,分为以下两类:*\n\n  1. Parent结果内嵌到现有的聚合分析结果中\n\n  * Derivative\n\n  * Moving Average\n\n  * Cumulative Sum\n\n  2. Sibling结果与现有聚合分析结果同级\n\n  * Max/Min/Avg/Sum Bucket\n\n  * Stats/Extended Stats Bucket\n\n  * Percentitles Bucket\n\n  **Sibing-Derivative**\n\n  *先根据job分桶并计算各个分桶的avg值，然后输出avg最小的桶名称和值*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"jobs\":{\n              \"terms\":{\n                  \"field\":\"job.keyword\",\n                  \"size\":10\n              },\n              \"aggs\":{\n                  \"avg_salary\":{\n                      \"avg\":{\n                          \"field\":\"salary\"\n                      }\n                  }\n              }\n          },\n          \"min_salary_by_job\":{\n              \"min_bucket\":{\n                  \"buckets_path\":\"jobs>avg_salary\"\n              }\n          }\n      }\n  }\n  ```\n\n  *先根据job分桶并计算各个分桶的avg值，然后对所有bucket进行stats分析*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"jobs\":{\n              \"terms\":{\n                  \"field\":\"job.keyword\",\n                  \"size\":10\n              },\n              \"aggs\":{\n                  \"avg_salary\":{\n                      \"avg\":{\n                          \"field\":\"salary\"\n                      }\n                  }\n              }\n          },\n          \"stats_salary_by_job\":{\n              \"stats_bucket\":{\n                  \"buckets_path\":\"jobs>avg_salary\"\n              }\n          }\n      }\n  }\n  ```\n\n  **Parent-Derivative**\n\n  *计算bucket的导数*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"birth\":{\n              \"date_histogram\":{\n                  \"field\":\"birth\",\n                  \"interval\":\"year\",\n                  \"min_doc_count\":0\n              },\n              \"aggs\":{\n                  \"avg_salary\":{\n                      \"avg\":{\n                          \"field\":\"salary\"\n                      }\n                  },\n                  \"derivative_avg_salary\":{\n                      \"derivative\":{\n                          \"buckets_path\":\"avg_salary\"\n                      }\n                  }\n              }\n          }\n      }\n  }\n  ```\n\n* Bucket+Metric聚合分析\n\n  *先根据job进行term分桶策略，然后再对每一个分桶进行range分桶策略*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n      \"size\":0,\n      \"aggs\":{\n          \"jobs\":{\n              \"terms\":{\n                  \"field\":\"job.keyword\",\n                  \"size\":10\n              },\n              \"aggs\":{\n                  \"age_range\":{\n                      \"range\":{\n                          \"field\":\"age\",\n                          \"ranges\":[\n                          {\"to\":20},\n                          {\"from\":20,\"to\":30},\n                          {\"from\":30}\n                          ]\n                      }\n                  }\n              }\n          }\n      }\n  }\n  ```\n* 搜索提示\n\n  *`_suggest`URI已经被`_search`所替代，使用`suggest`属性。*\n\n  ```bash\n  POST /userinfo/doc/_search\n  {\n    \"query\": {\n      \"match\": {\n        \"job\": \"web\"\n      }\n    },\n    \"suggest\": {\n      \"me_SUGGESTION\": {\n        \"text\": \"web\",\n        \"term\": {\n          \"field\": \"job\"\n        }\n      }\n    }\n  }\n  ```","tags":["Elasticsearch","Aggregations"],"categories":["大数据"]},{"title":"Elasticsearch(三)入门使用-索引与映射","url":"/2018/11/15/es_tutorial/","content":"\n> Elasticsearch: 6.4.2\n\n#### 1. 索引\n\n*索引是Elasticsearch中存储数据的一种逻辑结构。与关系型数据库对比：索引index类似于表，文档document类似于表中的行，字段field类似于表中的列，映射mapping类似于schema。索引是有分片组成的，可以分散到集群的多个节点中；分片实际上是Lucene的索引。*\n\n*在Elasticsearch6.x中一个索引只能包含一种类型的文档。*\n\n##### 1.1 索引操作\n\n创建Index\n\n```bash\nPUT /twitter\n```\n\n获取Index\n\n```bash\nGET /twitter\n```\n\n删除Index\n\n```bash\nDELETE /twitter\n```\n\nIndex是否存在\n\n```bash\nHEAD /twitter\n```\n\n打开/关闭Index\n\n```bash\nPOST /twitter/_close\nPOST /twitter/_open\n```\n\n##### 1.2 索引别名\n\n*索引别名是一个或多个缩影的另外一个名字。每个别名可以对应多个索引，每个索引也可以有多个别名。例如：我们将每天的日志创建一个索引，所有这些日志可以有有一个统一的索引名`log`；另外我们还可以将每年的日志的索引名再统一到一起起个别名`log2018`。*\n\n**不能使用已存在的索引名字作为别名。**\n\n创建别名\n\n```bash\nPOST /_aliases\n{\n    \"actions\" : [\n        { \"add\" : { \"index\" : \"test1\", \"alias\" : \"alias1\" } }\n    ]\n}\n```\n\n将索引从一个别名中删除\n\n```bash\nPOST /_aliases\n{\n    \"actions\" : [\n        { \"remove\" : { \"index\" : \"test1\", \"alias\" : \"alias1\" } }\n    ]\n}\n```\n\n获取所有别名\n\n```bash\nGET /_aliases\n```\n\n过滤别名\n\n*Elasticsearch支持以类似SQL数据中使用视图的方式使用别名。用户可以使用完整的查询DSL，并将查询应用在基于查询的统计、搜索、删除等操作上。*\n\n*确保在映射中已存在该字段。*\n\n```bash\n# 创建能够为客户12345返回数据的别名，则当使用该别名时，所有操作返回文档的clientId字段的值都是12345\nPOST /_aliases\n{\n    \"actions\" : [\n        {\n            \"add\" : {\n                 \"index\" : \"data\",\n                 \"alias\" : \"client\",\n                 \"filter\" : { \"term\" : { \"clientId\" : \"12345\" } }\n            }\n        }\n    ]\n}\n```\n\n别名和路由选择\n\n*对于命名为client的索引别名，使用12345、12346、12347作为路由值用于索引，并仅将12345用于查询*\n\n```bash\nPOST /_aliases\n{\n    \"actions\" : [\n        {\n            \"add\" : {\n                 \"index\" : \"data\",\n                 \"alias\" : \"client\",\n                 \"index_routing\" : \"12345,12346,12347\",\n                 \"search_routing\": \"12345\"\n            }\n        }\n    ]\n}\n```\n\n*当使用client别名对数据进行索引时，index_routing属性指定的值将被使用；当进行查询时，search_routing属性指定的值被使用*\n\n```bash\n# search_routing和查询中routing参数的共同值12345作为路由值\nGET /client/_search?q=test&routing=99999,12345\n```\n\n\n\n#### 2.  映射\n\n##### 2.1 映射操作\n\n*映射Mapping用于定义索引Index的结构。*\n\n> 6.0版本之后_all可能不再支持，因此最好使用copy_to\n\n\n\n**copy_to**: 允许创建定制的_all字段，将多个字段的值拷贝到查询时被当作单一字段使用的一组字段中。\n\n**_source**: 包含文本被索引阶段的原始JSON文档内容，它不能被索引，因此也不能被搜索，只在fetch请求时返回数据。\n\n创建或更新Mapping\n\n```bash\nPUT /twitter/_mapping/_doc \n{\n  \"properties\": {\n    \"email\": {\n      \"type\": \"keyword\"\n    }\n  }\n}\n```\n\n获取Mapping\n\n```bash\nGET /twitter/_mapping/_doc\n# 类型为_doc的所有索引的映射\nGET /_mapping/_doc\nGET /_all/_mapping/_doc\n# 所有索引所有类型的映射\nGET /_all/_mapping\nGET /_mapping\n```\n\n获取某字段的Mapping\n\n```bash\nGET /twitter/_mapping/_doc/field/email\n```\n\n##### 2.2 分析器\n\n**通常情况下索引和搜索时应该使用相同的分析器，针对使用分析器搜索特定字段，查找顺序如下：**\n\n* 当前查询语句中指定的分析器\n* 映射中`search_analyzer`参数定义的分析器\n* 映射中`analyzer`参数定义的分析器\n* 在索引settings部分`default_search`参数定义的分析器\n* 在索引settings部分`default`参数定义的分析器\n* 标准分析器\n\n*自定义分析器时需要在映射中添加一个settings部分，它保存创建索引时所需的信息。需要：*\n\n* 零个或多个字符过滤器\n* 一个分词器\n* 零个或多个词过滤器\n\n```bash\nPUT my_index\n{\n  \"settings\": {\n    \"analysis\": {\n      \"analyzer\": {\n        \"my_custom_analyzer\": {\n          \"type\":      \"custom\", \n          \"tokenizer\": \"standard\",\n          \"char_filter\": [\n            \"html_strip\"\n          ],\n          \"filter\": [\n            \"lowercase\",\n            \"asciifolding\"\n          ]\n        }\n      }\n    }\n  }\n}\n\n```\n\n##### 2.3 动态映射和模版\n\n*Elasticsearch最重要的特性之一就是动态映射，你不要先创建索引、定义映射，可以直接索引一个文档，此时elasticsearch自动帮你做这些工作。*\n\n*每一个模版定义了一个模式，用来与新建索引的名称进行比较；如果匹配则模版中定义的值被复制到索引结构的定义中。如果有多个模版匹配上了新建索引名称，所有模版都会被应用，并且后应用的模版会覆盖先应用的模版。其中的order参数可以控制模版的预期使用顺序。*\n\n```bash\nPUT my_index\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"dynamic_templates\": [\n        {\n          \"longs_as_strings\": {\n            \"match_mapping_type\": \"string\",\n            \"match\":   \"long_*\",\n            \"unmatch\": \"*_text\",\n            \"mapping\": {\n              \"type\": \"long\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n##### 2.4 路由\n\n**怎么索引和搜索**\n\n*默认情况下，elasticsearch计算文档ID的hash值及`hash值%主分片数`的值,基于后者的值将文档放到某个可用的主分片中，然后复制到副分片。*\n\n*搜索时将请求分发到索引的所有分片中，然后在每一个分片中根据条件查找文档并传递给某一个节点，该节点进行全局的过滤和排序等，并将最终结果返回给客户端。*\n\n路由选择可以控制文档和查询被转发到某一个特定的分片上。既可以在索引和搜索请求时使用routing参数，也可以在类型定义时使用_routing参数。\n\n```bash\nPUT /twitter/_doc/1?routing=kimchy\n{\n    \"user\" : \"kimchy\",\n    \"postDate\" : \"2009-11-15T14:12:12\",\n    \"message\" : \"trying out Elasticsearch\"\n}\n\nPUT my_index2\n{\n  \"mappings\": {\n    \"_doc\": {\n      \"_routing\": {\n        \"required\": true,\n        \"path\": \"id\"\n      }\n    }\n  }\n}\n```\n\n\n**可能发生的错误：`Fielddata is disabled on text fields by default`**.\n\n解决方法：\n\n*开启fileddata*\n\n```bash\n# 格式：/{index}/_mapping/{type};其中index：指存储文档的index； type：指文档的类型。\nPUT /megacorp/_mapping/employee\n{\n  \"properties\": {\n    \"interests\": {\n      \"type\": \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n```\n\n","tags":["Elasticsearch"],"categories":["大数据"]},{"title":"Elasticsearch(五)信息检索","url":"/2018/11/15/es_query/","content":"\n##### 1. 简单检索\n\n**由于自Elasticsearch6开始不再支持一个索引下存在多个类型的操作，因此也就没有了查询同一索引下多个类型的信息操作。**\n\n*from、size分别指定了从哪个结果开始返回、查询的结果集包含的最大文档数(默认是10)*\n\n* 查询指定索引指定类型下的信息\n\n  ```bash\n  GET /users/user/_search?q=name:bourne\n  ```\n\n* 查询多个或者所有索引，指定类型或多个类型下的信息\n\n  ```bash\n  GET /users,test/user/_search?q=name:bourne\n  GET /users,test/user, example/_search?q=name:bourne\n  GET /*/user/_search?q=name:bourne\n  ```\n\n* 查询所有索引所有类型下的信息\n\n  ```bash\n  GET /_search?q=name:bourne\n  ```\n\n##### 2. 基本检索\n\n* 设置不同字段的排序权重\n\n  *会影响文档的相关性得分，从而影响排序结果。*\n\n  ```bash\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"multi_match\": {\n        \"query\": \"bourne\",\n        \"fields\": [\"name^2\", \"message\"]\n      }\n    }\n  }\n  ```\n\n* 指定返回的字段子集\n\n  ```bash\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"match_all\": {}\n    },\n    \"_source\": [\"name\", \"message\"]\n  }\n  ```\n\n* term查询、terms查询、wildcard查询\n\n  *term查询: 仅匹配在给定字段有某个此项的文档，查询中的词项不再被解析属于精确查询;另外可以增加boost属性提升改term查询的重要性.*\n\n  *wildcard查询: 允许在要查询的内容中使用通配符(*****和**？**)`*\n\n  ```bash\n  # term查询\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"term\": {\n        \"name\": {\n          \"value\": \"bourne\"\n        }\n      }\n    }\n  }\n  # terms查询\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"terms\": {\n        \"name\": [\n          \"anna\",\n          \"bourne\"\n        ]\n      }\n    }\n  }\n  # wildcard查询\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"wildcard\": {\n        \"name\": {\n          \"value\": \"b*\"\n        }\n      }\n    }\n  }\n  ```\n\n* match查询、match_all查询、match_phrase查询\n\n  *match_phrase查询: 短语查询，可以通过指定slop参数定义在查询文本的词项之间应间隔多少个未知单词才视为成功,slop默认为0.*\n\n  ```bash\n  # match查询\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"match\": {\n        \"name\": \"bourne\"\n      }\n    },\n    \"sort\": [\n      {\n        \"created_at\": {\n          \"order\": \"desc\"\n        }\n      }\n    ],\n    \"size\": 20\n  }\n  # match_all查询\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"match_all\":{}\n    },\n    \"sort\": [\n      {\n        \"created_at\": {\n          \"order\": \"desc\"\n        }\n      }\n    ],\n    \"size\": 20\n  }\n  # match_phrase：对于查询文本the wife of Bourne;\n  # 如果slop不大于0，那么不会匹配成功.\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"match_phrase\": {\n        \"message\": {\n          \"query\": \"wife bourne\",\n          \"slop\": 1\n        }\n      }\n    }\n  }\n  ```\n\n* Prefix 、Range查询\n\n  *prefix查询：查找某个字段以给定前缀开头的文档，支持boost属性影响排序结果*\n\n  ```bash\n  # prefix查询\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"prefix\": {\n        \"name\": {\n          \"value\": \"j\"\n        }\n      }\n    }\n  }\n  # range查询：10天前至当前时间的文档\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"range\": {\n        \"created_at\": {\n          \"gte\": \"now-10d\",\n          \"lte\": \"now\"\n        }\n      }\n    }\n  }\n  ```\n\n* 跨字段查询multi_match\n\n  ```bash\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"multi_match\": {\n        \"query\": \"baby user\",\n        \"fields\": [\"name\", \"message\"]\n      }\n    }\n  }\n  ```","tags":["Elasticsearch","检索"],"categories":["大数据"]},{"title":"Elasticsearch(四)文档索引","url":"/2018/11/15/es_index/","content":"\n##### 1. 建立索引\n\n```bash\nPUT /users\n{\n  \"settings\": {\n    \"index\": {\n      \"number_of_shards\": 5,\n      \"number_of_replicas\": 1\n    }\n  }\n}\n```\n\n##### 2. 修改索引\n\n**索引的主分片数在索引创建之后就不能再修改，默认是5；副本分片是可以随时修改的。**\n\n```bash\nPUT /users/_settings\n{\n  \"index\": {\n    \"number_of_replicas\": 2\n  }\n}\n```\n\n* number_of_replicas: 设置索引的副本分片数\n* blocks.read_only: 如为true，则索引只能读，不能写和更新\n* blocks.read: 如为true，则禁止读取操作\n* blocks.write: 如为true，则禁止写操作\n* blocks.metadata: 如为true，则禁止对metadata操作\n\n##### 3. 查看索引配置信息\n\n```bash\nGET /users/_settings\n# 查看多个索引信息\nGET /users, roles/_settings\n# 使用通配符查看索引信息\nGET /use*/_settings\n# 查看所有索引信息\nGET /_all/_settings\n```\n\n##### 4. 删除索引\n\n```bash\nDELETE /users\n```\n\n##### 5. 插入数据\n\n*向索引名为users的索引插入一条type为user的数据*\n\n```bash\n# 由es自动生成id值\nPOST /users/user/\n{\n  \"name\": \"Alan\",\n  \"message\": \"create an example user\",\n  \"created_at\": \"2018-11-05T11:00:00\"\n}\n# 创建时指定id值\nPUT /users/user/1\n{\n  \"name\": \"Bourne\",\n  \"message\": \"create an example user\",\n  \"created_at\": \"2018-11-05T12:00:00\"\n}\n```\n\n##### 6. 使用映像mapping\n\n```bash\n# 通过mapping设置index中某个type下的field中的详细信息\nPUT /test\n{\n  \"mappings\": {\n    \"example\": {\n      \"properties\": {\n        \"user\": {\n          \"type\": \"text\",\n          \"index\": \"false\"\n        }\n      }\n    }\n  }\n}\n# 或者\nPUT /test/example/_mapping\n{\n  \"example\": {\n    \"properties\": {\n      \"message\": {\n        \"type\": \"text\",\n        \"index\": \"false\"\n      }\n    }\n  }\n}\n```\n\n**注意：es6.x以后移除了string类型，另外index的值只能时boolean了。**\n\n##### 7. 获取映像信息\n\n```bash\nGET /test/_mapping/example\n# 获取特定field的信息\nGET /test/_mapping/example/field/user\n```\n\n##### 8. 管理索引文件\n\n```bash\n# 打开\nPOST /users/_open\n# 关闭\nPOST /users/_close\n# 检测状态\nHEAD /users\n# 清除缓存\nPOST /users/_cache/clear\n```\n\n##### 9. 配置分析器\n\n```bash\n# 在索引users中基于standard分析器创建一个名为es_std的分析器\nPUT /users/_settings\n{\n  \"analysis\": {\n    \"analyzer\": {\n      \"es_std\": {\n        \"type\": \"custom\",\n        \"tokenizer\": \"standard\"\n      }\n    }\n  }\n}\n\n# 使用es_std进行分词\nGET /users/_analyze\n{\n  \"analyzer\": \"es_std\",\n  \"text\": \"joson bourne\"\n}\n```\n\n##### 10. 获取文档信息\n\n```bash\nGET /users/user/1\n# source过滤器\nGET /users/user/1?_source=false\n# 获取特定字段的值\nGET /users/user/1?_source=name\n```\n\n##### 11. 删除文档信息\n\n```bash\nDELETE /users/user/1\n```\n\n##### 12. 更新文档信息\n\n```bash\n# 第一种\nPOST /users/user/1/_update\n{\n  \"doc\": {\n   \"message\": \"bourne\" \n  }\n}\n# 第二种\nPOST /users/user/1/_update\n{\n  \"script\": \"ctx._source.message = \\\"Bourne1\\\"\"\n}\n```\n\n   **向数组添加元素**\n\n```bash\nPOST /users/user/1/_update\n{\n  \"script\" : {\n    \"source\": \"ctx._source.tags.add(params.new_tag)\",\n    \"params\" : {\n      \"new_tag\" : \"world\"\n   }\n  }\n}\n```\n\n**增加新字段**\n\n```bash\nPOST /users/user/1/_update\n{\n  \"script\" : {\n    \"source\": \"ctx._source.new_field=params.new_field\",\n    \"params\" : {\n      \"new_field\" : \"world\"\n   }\n  }\n}\n```\n\n**根据ID值找不到文档，则通过参数体中的upsert创建这个文档，并且加入新的字段；否则更新字段值。**\n\n```bash\nPOST /users/user/2/_update\n{\n  \"script\" : {\n    \"source\": \"ctx._source.count+=params.count\",\n    \"params\" : {\n      \"count\" : 4\n   }\n  },\n  \"upsert\": {\n     \"counter\": 1\n   }\n}\n```\n\n##### 13. 批量获取文档\n\n**通过指定`_index`、`_type`、`_id`获取不同索引不同类型的文档**\n\n```bash\nPOST /_mget\n{\n  \"docs\": [\n    {\n      \"_index\": \"users\",\n      \"_type\": \"user\",\n      \"_id\": \"1\"\n    },\n    {\n      \"_index\": \"roles\",\n      \"_type\": \"role\",\n      \"_id\": \"1\"\n    }\n    ]\n}\n```\n\n**获取同一索引同一类型下的文档，可以通过指定`ids`即可**\n\n```bash\nPOST /users/user/_mget\n{\n  \"ids\": [\"1\", \"2\"]\n}\n```\n","tags":["Elasticsearch","索引"],"categories":["大数据"]},{"title":"Elasticsearch(六)过滤器","url":"/2018/11/14/es_filter/","content":"\n* Bool filter\n\n  *有`must`、`should`、`must_not`三种逻辑操作;其中当仅存在should时则必须至少满足一个条件.*\n\n  ```bash\n  GET /books/book/_search\n  {\n    \"query\": {\n      \"bool\": {\n        \"must\": [\n          {\n            \"match\": {\n              \"name\": \"python\"\n            }\n          }\n        ],\n        \"should\": [\n          {\n            \"match\": {\n              \"title\": \"effective\"\n            }\n          }\n        ]\n      }\n    }\n  }\n  ```\n\n* Exists filter\n\n *`exists filter`过滤搜索结果,使其必须存在某个指定的字段.*\n\n ```bash\n# exists filter\nGET /books/book/_search\n{\n  \"query\": {\n    \"exists\": {\n      \"field\": \"name\"\n    }\n  }\n}\n ```\n\n**Null值的讨论:**\n\n*假如索引`test`中存在类型为`test`的如下文档:*\n\n```bash\n{\"user\": \"\"} # 1\n{\"user\": []} # 2\n{\"user\": null} # 3\n{\"user\": [null]} # 4\n{\"user\": \"-\"} # 5\n{\"user\": \"foo\"} # 6\n{\"person\": \"bar\"} # 7\n```\n\n*对于如下的查询,将会返回`1、5、6`三条数据.*\n\n*默认情况下:`2、3、4、7`四种情况都被认为是`null`值而被过滤,最终不可被搜索.*\n\n```bash\nGET /test/test/_search\n{\n  \"query\": {\n    \"exists\": {\n      \"field\": \"user\"\n    }\n  }\n}\n```\n\n*可以在创建索引前,通过设置`mapping`中`null_value`属性的值,从而让`3、4`这两种情况文档可被搜索.*\n\n* Type filter\n\n  *返回指定type的文档.*\n\n  ```bash\n  GET /books/_search\n  {\n    \"query\": {\n      \"type\": {\n        \"value\": \"book\"\n      }\n    }\n  }\n  ```\n\n* Match_all filter\n\n  *选中全部数据,相当于SQL语句中的*`select * from book`.\n\n  ```bash\n  GET /books/_search\n  {\n    \"query\": {\n      \"type\": {\n        \"value\": \"book\"\n      }\n    }\n  }\n  ```\n\n* Query filter\n\n  *由`AND`、`OR`两种逻辑*\n\n  ```bash\n  GET /books/book/_search\n  {\n    \"query\": {\n      \"query_string\": {\n        \"default_field\": \"name\",\n        \"query\": \"python OR java\"  # \"python AND java\"\n      }\n    }\n  }\n  ```\n\n* 结果排序\n\n  *默认按照相关度得分`_score`倒排,还可以通过`sort`针对特定字段正排(`asc`)和倒排(`desc`);另外还可以在`sort`中指定`_last`将无值的结果放在检索集的最后.*\n\n  ```bash\n  GET /users/user/_search\n  {\n    \"query\": {\n      \"match\": {\n        \"name\": \"bourne\"\n      }\n    },\n    \"sort\": [\n      {\n        \"created_at\": {\n          \"order\": \"desc\",\n          \"missing\": \"_last\"\n        }\n      }\n    ]\n  }\n  ```\n","tags":["Elasticsearch","过滤器"],"categories":["大数据"]},{"title":"Elasticsearch(二)使用dokcer-compose编排Elasticsearch服务","url":"/2018/11/05/es_docker_compose/","content":"\n> Elasticsearch: 6.4.2\n> Kibana: 6.4.2\n\n*在一台主机上使用docker-compose编排es的单主机多容器集群和kibana服务是非常有必要的,可以集中管理这些服务了.*\n\n`docker-compose.yml`\n```yaml\nversion: '3'\nservices:\n  node1:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2\n    container_name: node1\n    environment:\n      - node.name=es01\n      - cluster.name=es-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    ulimits:\n      nproc: 65535\n      memlock:\n        soft: -1\n        hard: -1\n    cap_add:\n      - ALL\n    privileged: true\n    deploy:\n      mode: global\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n    volumes:\n      - ./node1/data:/usr/share/elasticsearch/data\n    ports:\n      - 9200:9200\n      - 9300:9300\n  node2:\n    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.2\n    container_name: node2\n    environment:\n      - node.name=es02\n      - cluster.name=es-cluster\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n      - \"discovery.zen.ping.unicast.hosts=node1\"\n    ulimits:\n      nproc: 65535\n      memlock:\n        soft: -1\n        hard: -1\n    cap_add:\n      - ALL\n    privileged: true\n    deploy:\n      mode: global\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n    volumes:\n      - ./node2/data:/usr/share/elasticsearch/data\n    ports:\n      - 9201:9200\n      - 9301:9300\n  kibana:\n    image: docker.elastic.co/kibana/kibana:6.4.2\n    container_name: kibana\n    environment:\n      SERVER_NAME: localhost\n      ELASTICSEARCH_URL: http://node1:9200\n    ports:\n      - 5601:5601\n    ulimits:\n      nproc: 65535\n      memlock:\n        soft: -1\n        hard: -1\n    cap_add:\n      - ALL\n    deploy:\n      mode: global\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n```\n**使用如下命令启动:**\n```bash\ndocker-compose up -d\n```\n**浏览器打开http://localhost:5601即可监控集群状态.**","tags":["Elasticsearch"],"categories":["大数据"]},{"title":"Elasticsearch(一) Docker 集群部署—单机多容器实例","url":"/2018/11/05/es_cluster/","content":"\n> Elasticsearch: 6.4.2\n>\n> 环境：在Mac上搭建的单机多容器实例：1个master节点，一个slave节点\n\n\n\n#### 1. 以Docker形式安装Elasticsearch\n\n拉去镜像：\n\n```bash\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:6.4.2\n```\n\n#### 2. 启动\n\n启动master节点：\n\n```bash\ndocker run -d -p 9200:9200 -p 9300:9300 --name elas1 -h elas1  -e cluster.name=lookout-es -e ES_JAVA_OPTS=\"-Xms512m -Xmx512m\" -e xpack.security.enabled=false docker.elastic.co/elasticsearch/elasticsearch:6.4.2\n```\n\n启动slave节点：\n\n```bash\ndocker run -d -p 9211:9200 -p 9311:9300 --link elas1  --name elas2 -e cluster.name=lookout-es -e ES_JAVA_OPTS=\"-Xms512m -Xmx512m\" -e xpack.security.enabled=false -e discovery.zen.ping.unicast.hosts=elas1 docker.elastic.co/elasticsearch/elasticsearch:6.4.2\n```\n\n\n\n| 参数   | 作用                               |\n| ------ | ---------------------------------- |\n| -p     | 端口映射(主机:)                    |\n| --name | 启动容器的名字                     |\n| -h     | 容器主机名                         |\n| -e     | 指定docker内环境变量参数           |\n| -d     | 程序后台运行                       |\n| --link | 相当于在容器中配置了link对象的host |\n\n\n\n#### 3. 验证\n\n##### 3.1、检查节点状态\n\n```bash\nhttp://localhost:9211/\n```\n\n##### 3.2、检查集群状态\n\n```bash\nhttp://localhost:9200/_cat/health?v\n```\n\n#### 4. 监控\n\n**使用官方提供的Kibana，以docker方式安装:**\n\n```bash\ndocker pull docker.elastic.co/kibana/kibana:6.4.2\n```\n\n**运行Kibana**\n\n```bash\ndocker run -d --name kibana  --link elas1 -e ELASTICSEARCH_URL=http://elas1:9200 -p 5601:5601 docker.elastic.co/kibana/kibana:6.4.2\n```\n\n**通过http://localhost:5601进行访问.**\n\n ","tags":["Elasticsearch"],"categories":["大数据"]},{"title":"使用Hexo+Github Page搭建个人博客","url":"/2018/11/03/hexo/","content":"[Hexoz中文文档](https://hexo.io/zh-cn/docs/index.html)\n\n## Quick Start\n\n### Github\n在个人`GitHub`上创建名字形式为`xxx.github.io`的repository,并clone到本地.\n\n### 基本配置\n```yml\ntitle: 名称\nsubtitle: 子名称\ndescription: 描述\nkeywords: 关键词\nauthor: 作者\nlanguage: 本地化(查看themes/hexo-theme-matery/languages)\n```\n\n### 更换主题\n*本博客使用的是`hexo-theme-matery`主题,在此非常感谢`blinkfox`的开源贡献.*\n1. 下载主题\n```bash\ngit clone https://github.com/blinkfox/hexo-theme-matery.git themes/hexo-theme-matery\n```\n2. 在根目录下修改`_config.yml`文件\n```yml\ntheme: hexo-theme-matery\n```\n\n### 新建categories\n`categories`页是用来展示所有分类的页面，如果在你的博客`source`目录下还没有`categories/index.md`文件，使用如下命令创建:\n\n``` bash\n$ hexo new page \"categories\"\n```\n编辑你刚刚新建的页面文件`/source/categories/index.md`，添加以下内容:\n```yml\n---\ntitle: 分类\ndate: 2018-11-03 15:32:23\ntype: \"categories\"\nlayout: \"categories\"\n---\n```\n\n### 新建tags\n`tags`页是用来展示所有标签的页面，如果在你的博客`source`目录下还没有`tags/index.md`文件，使用如下命令创建:\n\n``` bash\n$ hexo new page \"tags\"\n```\n编辑你刚刚新建的页面文件`/source/tags/index.md`，添加以下内容:\n```yml\n---\ntitle: 标签\ndate: 2018-11-03 14:45:54\ntype: \"tags\"\nlayout: \"tags\"\n---\n```\n\n### 新建about\n`about`页是用来展示关于我和我的博客信息的页面，如果在你的博客`source`目录下还没有`about/index.md`文件，使用如下命令创建:\n\n``` bash\n$ hexo new page \"about\"\n```\n编辑你刚刚新建的页面文件`/source/about/index.md`，添加以下内容:\n```yml\n---\ntitle: about\ndate: 2018-11-03 15:48:12\ntype: \"about\"\nlayout: \"about\"\n---\n```\n\n### 代码高亮\n由于Hexo默认的高亮主题不好用,所以我使用了`hexo-prism-plugin`的`Hexo`插件来做代码高亮,安装命令如下:\n```bash\nnpm install hexo-prism-plugin --save\n```\n然后修改根目录下的`_config.yml`文件,修改如下:\n```yml\nhighlight:\n  enable: false\n\nprism_plugin:\n  mode: 'preprocess'    # realtime/preprocess\n  theme: 'tomorrow'\n  line_number: false    # default false\n  custom_css:\n```\n\n### 站内搜索\n使用`Hexo`插件`hexo-generator-search`来做站内搜素,安装命令如下:\n```bash\nnpm install hexo-generator-search --save\n```\n然后修改根目录下的`_config.yml`文件,修改如下:\n```yml\nsearch:\n  path: search.json\n  field: post\n```\n\n### 添加RSS订阅支持\n使用`Hexo`插件`hexo-generator-feed`,安装命令如下:\n```bash\nnpm install hexo-generator-feed --save\n```\n然后修改根目录下的`_config.yml`文件,修改如下:\n```yml\nfeed:\n  type: atom\n  path: atom.xml\n  limit: 20\n  hub:\n  content:\n  content_limit: 140\n  content_limit_delim: ' '\n  order_by: -date\n```\n\n### 修改社交链接\n修改`themes/hexo-theme-matery/layout/social-link.ejs`,将社交链接改成自己的相关信息即可.\n\n### 文章的Front-matter示例\n```bash\n---\ntitle: Hexo\ndate: 2018-09-07 09:25:00\ntop: true # 如果top值为true，则会是首页推荐文章\ncategories: 随笔\ntags:\n  - Hexo \n  - Markdown\n---\n```\n\n### 本地运行\n**运行前先在根目录下执行命令`npm install`**\n``` bash\n$ hexo server | hexo s\n```\n\n### 生成静态文件\n\n``` bash\n$ hexo generate | hexo g\n```\n\n### 部署\n**请先在Github配置SSH key,然后在根目录的`_config.yml`文件中进行如下的配置:**\n```yml\ndeploy:\n  type: git\n  repo: 个人博客在GitHub上的repository,如 git@github.com:xxx/xxx.github.io.git \n  branch: master\n```\n下载`Hexo`的`git`插件,安装命令如下:\n```bash\nnpm install --save hexo-deployer-git\n```\n\n``` bash\n$ hexo deploy | hexo d\n```\n","tags":["Hexo"],"categories":["随笔"]}]